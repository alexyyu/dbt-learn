2019-02-07 15:47:06,638 (MainThread): Tracking: tracking
2019-02-07 15:47:06,639 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025F89D32240>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025F89D323C8>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025F89D32630>]}
2019-02-07 15:47:06,780 (MainThread): Warning: No packages were found in packages.yml
2019-02-07 15:47:06,781 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025F89B5EA20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025F874A94A8>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025F89D1DB00>]}
2019-02-07 15:47:06,884 (MainThread): Flushing usage events
2019-02-07 15:47:34,283 (MainThread): Tracking: tracking
2019-02-07 15:47:34,283 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AD1CFD2668>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AD1CFD27F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AD1CFD22E8>]}
2019-02-07 15:47:34,815 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AD1CFBD7F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AD1CFBD7B8>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AD1CFBDDD8>]}
2019-02-07 15:47:34,902 (MainThread): Flushing usage events
2019-02-07 15:47:34,903 (MainThread): Encountered an error:
2019-02-07 15:47:34,903 (MainThread): 'dict' object has no attribute 'packages'
2019-02-07 15:47:34,906 (MainThread): Traceback (most recent call last):
  File "c:\users\yichy\appdata\local\programs\python\python36\lib\site-packages\dbt\main.py", line 76, in main
    results, succeeded = handle_and_check(args)
  File "c:\users\yichy\appdata\local\programs\python\python36\lib\site-packages\dbt\main.py", line 126, in handle_and_check
    task, res = run_from_args(parsed)
  File "c:\users\yichy\appdata\local\programs\python\python36\lib\site-packages\dbt\main.py", line 181, in run_from_args
    results = run_from_task(task, cfg, parsed)
  File "c:\users\yichy\appdata\local\programs\python\python36\lib\site-packages\dbt\main.py", line 189, in run_from_task
    result = task.run()
  File "c:\users\yichy\appdata\local\programs\python\python36\lib\site-packages\dbt\task\deps.py", line 440, in run
    sub_deps.incorporate_from_yaml(target_config.packages.packages)
AttributeError: 'dict' object has no attribute 'packages'

2019-02-07 15:49:14,600 (MainThread): Tracking: tracking
2019-02-07 15:49:14,600 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000220C38CF518>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000220C38CF9B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000220C38CF898>]}
2019-02-07 15:49:15,421 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000220C38C50B8>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000220C38C5198>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000220C38C5D30>]}
2019-02-07 15:49:15,516 (MainThread): Flushing usage events
2019-02-07 15:49:15,517 (MainThread): Encountered an error:
2019-02-07 15:49:15,517 (MainThread): 'dict' object has no attribute 'packages'
2019-02-07 15:49:15,519 (MainThread): Traceback (most recent call last):
  File "c:\users\yichy\appdata\local\programs\python\python36\lib\site-packages\dbt\main.py", line 76, in main
    results, succeeded = handle_and_check(args)
  File "c:\users\yichy\appdata\local\programs\python\python36\lib\site-packages\dbt\main.py", line 126, in handle_and_check
    task, res = run_from_args(parsed)
  File "c:\users\yichy\appdata\local\programs\python\python36\lib\site-packages\dbt\main.py", line 181, in run_from_args
    results = run_from_task(task, cfg, parsed)
  File "c:\users\yichy\appdata\local\programs\python\python36\lib\site-packages\dbt\main.py", line 189, in run_from_task
    result = task.run()
  File "c:\users\yichy\appdata\local\programs\python\python36\lib\site-packages\dbt\task\deps.py", line 440, in run
    sub_deps.incorporate_from_yaml(target_config.packages.packages)
AttributeError: 'dict' object has no attribute 'packages'

2019-02-07 15:52:41,632 (MainThread): Tracking: tracking
2019-02-07 15:52:41,632 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DDF7AD2668>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DDF7AD27F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DDF7AD22E8>]}
2019-02-07 15:52:42,125 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DDF7ABCE48>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DDF7ABC160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DDF7ABC9B0>]}
2019-02-07 15:52:42,217 (MainThread): Flushing usage events
2019-02-07 15:52:42,217 (MainThread): Encountered an error:
2019-02-07 15:52:42,217 (MainThread): 'dict' object has no attribute 'packages'
2019-02-07 15:52:42,219 (MainThread): Traceback (most recent call last):
  File "c:\users\yichy\appdata\local\programs\python\python36\lib\site-packages\dbt\main.py", line 76, in main
    results, succeeded = handle_and_check(args)
  File "c:\users\yichy\appdata\local\programs\python\python36\lib\site-packages\dbt\main.py", line 126, in handle_and_check
    task, res = run_from_args(parsed)
  File "c:\users\yichy\appdata\local\programs\python\python36\lib\site-packages\dbt\main.py", line 181, in run_from_args
    results = run_from_task(task, cfg, parsed)
  File "c:\users\yichy\appdata\local\programs\python\python36\lib\site-packages\dbt\main.py", line 189, in run_from_task
    result = task.run()
  File "c:\users\yichy\appdata\local\programs\python\python36\lib\site-packages\dbt\task\deps.py", line 440, in run
    sub_deps.incorporate_from_yaml(target_config.packages.packages)
AttributeError: 'dict' object has no attribute 'packages'

2019-02-07 15:53:02,480 (MainThread): Tracking: tracking
2019-02-07 15:53:02,481 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000191D2970940>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000191D2970D68>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000191D2970198>]}
2019-02-07 15:53:03,002 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000191D29662B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000191D2966C18>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000191D2966DD8>]}
2019-02-07 15:53:03,103 (MainThread): Flushing usage events
2019-02-07 15:53:03,103 (MainThread): Encountered an error:
2019-02-07 15:53:03,103 (MainThread): 'dict' object has no attribute 'packages'
2019-02-07 15:53:03,105 (MainThread): Traceback (most recent call last):
  File "c:\users\yichy\appdata\local\programs\python\python36\lib\site-packages\dbt\main.py", line 76, in main
    results, succeeded = handle_and_check(args)
  File "c:\users\yichy\appdata\local\programs\python\python36\lib\site-packages\dbt\main.py", line 126, in handle_and_check
    task, res = run_from_args(parsed)
  File "c:\users\yichy\appdata\local\programs\python\python36\lib\site-packages\dbt\main.py", line 181, in run_from_args
    results = run_from_task(task, cfg, parsed)
  File "c:\users\yichy\appdata\local\programs\python\python36\lib\site-packages\dbt\main.py", line 189, in run_from_task
    result = task.run()
  File "c:\users\yichy\appdata\local\programs\python\python36\lib\site-packages\dbt\task\deps.py", line 440, in run
    sub_deps.incorporate_from_yaml(target_config.packages.packages)
AttributeError: 'dict' object has no attribute 'packages'

2019-02-07 15:53:28,819 (MainThread): Tracking: tracking
2019-02-07 15:53:28,820 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D7C1E12B38>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D7C1E12240>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D7C1E12470>]}
2019-02-07 15:53:28,918 (MainThread): Loading dependency project from c:\users\yichy\appdata\local\programs\python\python36\lib\site-packages\dbt\include
2019-02-07 15:53:28,926 (MainThread): Loading dependency project from C:\users\yichy\dbt-learn\dbt_modules
2019-02-07 15:53:28,936 (MainThread): Parsing macros\core.sql
2019-02-07 15:53:28,945 (MainThread): Parsing macros\adapters\bigquery.sql
2019-02-07 15:53:28,952 (MainThread): Parsing macros\adapters\common.sql
2019-02-07 15:53:28,966 (MainThread): Parsing macros\adapters\redshift.sql
2019-02-07 15:53:28,975 (MainThread): Parsing macros\adapters\snowflake.sql
2019-02-07 15:53:28,978 (MainThread): Parsing macros\catalog\postgres_catalog.sql
2019-02-07 15:53:28,980 (MainThread): Parsing macros\catalog\redshift_catalog.sql
2019-02-07 15:53:28,989 (MainThread): Parsing macros\catalog\snowflake_catalog.sql
2019-02-07 15:53:28,992 (MainThread): Parsing macros\etc\bigquery.sql
2019-02-07 15:53:28,994 (MainThread): Parsing macros\etc\datetime.sql
2019-02-07 15:53:29,006 (MainThread): Parsing macros\etc\get_custom_schema.sql
2019-02-07 15:53:29,009 (MainThread): Parsing macros\materializations\helpers.sql
2019-02-07 15:53:29,018 (MainThread): Parsing macros\materializations\archive\archive.sql
2019-02-07 15:53:29,047 (MainThread): Parsing macros\materializations\common\merge.sql
2019-02-07 15:53:29,052 (MainThread): Parsing macros\materializations\incremental\bigquery_incremental.sql
2019-02-07 15:53:29,058 (MainThread): Parsing macros\materializations\incremental\incremental.sql
2019-02-07 15:53:29,068 (MainThread): Parsing macros\materializations\seed\bigquery.sql
2019-02-07 15:53:29,071 (MainThread): Parsing macros\materializations\seed\seed.sql
2019-02-07 15:53:29,087 (MainThread): Parsing macros\materializations\table\bigquery_table.sql
2019-02-07 15:53:29,097 (MainThread): Parsing macros\materializations\table\snowflake_table.sql
2019-02-07 15:53:29,106 (MainThread): Parsing macros\materializations\table\table.sql
2019-02-07 15:53:29,115 (MainThread): Parsing macros\materializations\view\bq_snowflake_view.sql
2019-02-07 15:53:29,126 (MainThread): Parsing macros\materializations\view\view.sql
2019-02-07 15:53:29,134 (MainThread): Parsing macros\operations\catalog\get_catalog.sql
2019-02-07 15:53:29,135 (MainThread): Parsing macros\operations\relations\get_relations.sql
2019-02-07 15:53:29,137 (MainThread): Parsing macros\relations\postgres_relations.sql
2019-02-07 15:53:29,139 (MainThread): Parsing macros\relations\redshift_relations.sql
2019-02-07 15:53:29,141 (MainThread): Parsing macros\schema_tests\accepted_values.sql
2019-02-07 15:53:29,143 (MainThread): Parsing macros\schema_tests\not_null.sql
2019-02-07 15:53:29,145 (MainThread): Parsing macros\schema_tests\relationships.sql
2019-02-07 15:53:29,147 (MainThread): Parsing macros\schema_tests\unique.sql
2019-02-07 15:53:29,154 (MainThread): Parsing macros\core.sql
2019-02-07 15:53:29,158 (MainThread): Parsing macros\adapters\bigquery.sql
2019-02-07 15:53:29,161 (MainThread): Parsing macros\adapters\common.sql
2019-02-07 15:53:29,170 (MainThread): Parsing macros\adapters\redshift.sql
2019-02-07 15:53:29,176 (MainThread): Parsing macros\adapters\snowflake.sql
2019-02-07 15:53:29,178 (MainThread): Parsing macros\catalog\postgres_catalog.sql
2019-02-07 15:53:29,179 (MainThread): Parsing macros\catalog\redshift_catalog.sql
2019-02-07 15:53:29,185 (MainThread): Parsing macros\catalog\snowflake_catalog.sql
2019-02-07 15:53:29,187 (MainThread): Parsing macros\etc\bigquery.sql
2019-02-07 15:53:29,188 (MainThread): Parsing macros\etc\datetime.sql
2019-02-07 15:53:29,195 (MainThread): Parsing macros\etc\get_custom_schema.sql
2019-02-07 15:53:29,197 (MainThread): Parsing macros\materializations\helpers.sql
2019-02-07 15:53:29,203 (MainThread): Parsing macros\materializations\archive\archive.sql
2019-02-07 15:53:29,225 (MainThread): Parsing macros\materializations\common\merge.sql
2019-02-07 15:53:29,228 (MainThread): Parsing macros\materializations\incremental\bigquery_incremental.sql
2019-02-07 15:53:29,234 (MainThread): Parsing macros\materializations\incremental\incremental.sql
2019-02-07 15:53:29,242 (MainThread): Parsing macros\materializations\seed\bigquery.sql
2019-02-07 15:53:29,245 (MainThread): Parsing macros\materializations\seed\seed.sql
2019-02-07 15:53:29,258 (MainThread): Parsing macros\materializations\table\bigquery_table.sql
2019-02-07 15:53:29,266 (MainThread): Parsing macros\materializations\table\snowflake_table.sql
2019-02-07 15:53:29,276 (MainThread): Parsing macros\materializations\table\table.sql
2019-02-07 15:53:29,284 (MainThread): Parsing macros\materializations\view\bq_snowflake_view.sql
2019-02-07 15:53:29,291 (MainThread): Parsing macros\materializations\view\view.sql
2019-02-07 15:53:29,299 (MainThread): Parsing macros\operations\catalog\get_catalog.sql
2019-02-07 15:53:29,301 (MainThread): Parsing macros\operations\relations\get_relations.sql
2019-02-07 15:53:29,302 (MainThread): Parsing macros\relations\postgres_relations.sql
2019-02-07 15:53:29,303 (MainThread): Parsing macros\relations\redshift_relations.sql
2019-02-07 15:53:29,304 (MainThread): Parsing macros\schema_tests\accepted_values.sql
2019-02-07 15:53:29,306 (MainThread): Parsing macros\schema_tests\not_null.sql
2019-02-07 15:53:29,307 (MainThread): Parsing macros\schema_tests\relationships.sql
2019-02-07 15:53:29,308 (MainThread): Parsing macros\schema_tests\unique.sql
2019-02-07 15:53:29,311 (MainThread): Parsing model.dbt_learn.my_first_dbt_model
2019-02-07 15:53:29,327 (MainThread): Parsing model.dbt_learn.util_calendar
2019-02-07 15:53:29,362 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.my_new_package.example

2019-02-07 15:53:29,366 (MainThread): Found 2 models, 0 tests, 0 archives, 0 analyses, 94 macros, 2 operations, 0 seed files
2019-02-07 15:53:29,368 (MainThread): 
2019-02-07 15:53:29,368 (MainThread): Acquiring new redshift connection "master".
2019-02-07 15:53:29,368 (MainThread): Opening a new connection (0 currently allocated)
2019-02-07 15:53:29,369 (MainThread): Connecting to Redshift using 'database' credentials
2019-02-07 15:53:30,095 (MainThread): Using redshift connection "master".
2019-02-07 15:53:30,095 (MainThread): On master: select tablename as name, schemaname as schema, 'table' as type from pg_tables
        where schemaname ilike 'dbt_alex'
        union all
        select viewname as name, schemaname as schema, 'view' as type from pg_views
        where schemaname ilike 'dbt_alex'
2019-02-07 15:53:30,144 (MainThread): SQL status: SELECT in 0.05 seconds
2019-02-07 15:53:30,188 (MainThread): Acquiring new redshift connection "get_relations_data".
2019-02-07 15:53:30,189 (MainThread): Opening a new connection (1 currently allocated)
2019-02-07 15:53:30,189 (MainThread): Connecting to Redshift using 'database' credentials
2019-02-07 15:53:30,333 (MainThread): Using redshift connection "get_relations_data".
2019-02-07 15:53:30,333 (MainThread): On get_relations_data: BEGIN
2019-02-07 15:53:30,371 (MainThread): SQL status: BEGIN in 0.04 seconds
2019-02-07 15:53:30,372 (MainThread): Using redshift connection "get_relations_data".
2019-02-07 15:53:30,372 (MainThread): On get_relations_data: -- 
    with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg_%'
    ),
    relationships as (
        select
            referenced_class.name as referenced_name,
            referenced_class.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced_class.kind as kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        join dependency on relation.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            referenced_class.kind in ('r', 'v') and
            (referenced_class.name != dependent_class.name or
             referenced_class.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2019-02-07 15:53:30,420 (MainThread): SQL status: SELECT in 0.05 seconds
2019-02-07 15:53:30,429 (MainThread): On get_relations_data: ROLLBACK
2019-02-07 15:53:30,449 (MainThread): Using redshift connection "master".
2019-02-07 15:53:30,449 (MainThread): On master: select distinct nspname from pg_namespace
2019-02-07 15:53:30,467 (MainThread): SQL status: SELECT in 0.02 seconds
2019-02-07 15:53:30,467 (MainThread): 15:53:30 | Concurrency: 4 threads (target='dev')
2019-02-07 15:53:30,467 (MainThread): 15:53:30 | 
2019-02-07 15:53:30,471 (Thread-1): 15:53:30 | 1 of 2 START view model dbt_alex.util_calendar....................... [RUN]
2019-02-07 15:53:30,472 (Thread-1): Compiling model.dbt_learn.util_calendar
2019-02-07 15:53:30,471 (Thread-2): 15:53:30 | 2 of 2 START view model dbt_alex.my_first_dbt_model.................. [RUN]
2019-02-07 15:53:30,477 (Thread-2): Compiling model.dbt_learn.my_first_dbt_model
2019-02-07 15:53:30,480 (Thread-2): Writing injected SQL for node "model.dbt_learn.my_first_dbt_model"
2019-02-07 15:53:30,484 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '25a24275-f946-4e25-9b76-9e0b41880ba2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D7C1E83B00>]}
2019-02-07 15:53:30,523 (Thread-2): Acquiring new redshift connection "my_first_dbt_model".
2019-02-07 15:53:30,523 (Thread-2): Re-using an available connection from the pool.
2019-02-07 15:53:30,523 (Thread-2): Using redshift connection "my_first_dbt_model".
2019-02-07 15:53:30,524 (Thread-2): On my_first_dbt_model: BEGIN
2019-02-07 15:53:30,565 (Thread-2): SQL status: BEGIN in 0.04 seconds
2019-02-07 15:53:30,566 (Thread-2): Using redshift connection "my_first_dbt_model".
2019-02-07 15:53:30,566 (Thread-2): On my_first_dbt_model: drop view if exists "dbt_alex"."my_first_dbt_model__dbt_tmp" cascade
2019-02-07 15:53:30,588 (Thread-2): SQL status: DROP VIEW in 0.02 seconds
2019-02-07 15:53:30,588 (Thread-2): On my_first_dbt_model: COMMIT
2019-02-07 15:53:30,588 (Thread-2): Using redshift connection "my_first_dbt_model".
2019-02-07 15:53:30,588 (Thread-2): On my_first_dbt_model: COMMIT
2019-02-07 15:53:30,589 (Thread-1): 15:53:30 | 1 of 2 ERROR creating view model dbt_alex.util_calendar.............. [ERROR in 0.01s]
2019-02-07 15:53:30,771 (Thread-2): SQL status: COMMIT in 0.18 seconds
2019-02-07 15:53:30,771 (Thread-2): Using redshift connection "my_first_dbt_model".
2019-02-07 15:53:30,771 (Thread-2): On my_first_dbt_model: BEGIN
2019-02-07 15:53:30,790 (Thread-2): SQL status: BEGIN in 0.02 seconds
2019-02-07 15:53:30,790 (Thread-2): On my_first_dbt_model: COMMIT
2019-02-07 15:53:30,790 (Thread-2): Using redshift connection "my_first_dbt_model".
2019-02-07 15:53:30,790 (Thread-2): On my_first_dbt_model: COMMIT
2019-02-07 15:53:30,809 (Thread-2): SQL status: COMMIT in 0.02 seconds
2019-02-07 15:53:30,810 (Thread-2): Using redshift connection "my_first_dbt_model".
2019-02-07 15:53:30,810 (Thread-2): On my_first_dbt_model: BEGIN
2019-02-07 15:53:30,830 (Thread-2): SQL status: BEGIN in 0.02 seconds
2019-02-07 15:53:30,830 (Thread-2): Using redshift connection "my_first_dbt_model".
2019-02-07 15:53:30,830 (Thread-2): On my_first_dbt_model: drop view if exists "dbt_alex"."my_first_dbt_model__dbt_backup" cascade
2019-02-07 15:53:30,852 (Thread-2): SQL status: DROP VIEW in 0.02 seconds
2019-02-07 15:53:30,852 (Thread-2): On my_first_dbt_model: COMMIT
2019-02-07 15:53:30,852 (Thread-2): Using redshift connection "my_first_dbt_model".
2019-02-07 15:53:30,852 (Thread-2): On my_first_dbt_model: COMMIT
2019-02-07 15:53:30,943 (Thread-2): SQL status: COMMIT in 0.09 seconds
2019-02-07 15:53:30,943 (Thread-2): Using redshift connection "my_first_dbt_model".
2019-02-07 15:53:30,944 (Thread-2): On my_first_dbt_model: BEGIN
2019-02-07 15:53:30,963 (Thread-2): SQL status: BEGIN in 0.02 seconds
2019-02-07 15:53:30,979 (Thread-2): Writing runtime SQL for node "model.dbt_learn.my_first_dbt_model"
2019-02-07 15:53:30,985 (Thread-2): Using redshift connection "my_first_dbt_model".
2019-02-07 15:53:30,986 (Thread-2): On my_first_dbt_model: 

  create view "dbt_alex"."my_first_dbt_model__dbt_tmp" as (
    -- Welcome to your first dbt model!
-- Did you know that you can also configure models directly within
-- the SQL file? This will override configurations stated in dbt_project.yml

-- Try changing 'view' to 'table', then re-running dbt



select 1 as id
  ) ;
2019-02-07 15:53:31,070 (Thread-2): SQL status: CREATE VIEW in 0.08 seconds
2019-02-07 15:53:31,073 (Thread-2): Using redshift connection "my_first_dbt_model".
2019-02-07 15:53:31,073 (Thread-2): On my_first_dbt_model: alter table "dbt_alex"."my_first_dbt_model" rename to "my_first_dbt_model__dbt_backup"
2019-02-07 15:53:31,094 (Thread-2): SQL status: ALTER TABLE in 0.02 seconds
2019-02-07 15:53:31,095 (Thread-2): Using redshift connection "my_first_dbt_model".
2019-02-07 15:53:31,095 (Thread-2): On my_first_dbt_model: alter table "dbt_alex"."my_first_dbt_model__dbt_tmp" rename to "my_first_dbt_model"
2019-02-07 15:53:31,115 (Thread-2): SQL status: ALTER TABLE in 0.02 seconds
2019-02-07 15:53:31,115 (Thread-2): On my_first_dbt_model: COMMIT
2019-02-07 15:53:31,115 (Thread-2): Using redshift connection "my_first_dbt_model".
2019-02-07 15:53:31,115 (Thread-2): On my_first_dbt_model: COMMIT
2019-02-07 15:53:31,240 (Thread-2): SQL status: COMMIT in 0.12 seconds
2019-02-07 15:53:31,240 (Thread-2): Using redshift connection "my_first_dbt_model".
2019-02-07 15:53:31,240 (Thread-2): On my_first_dbt_model: BEGIN
2019-02-07 15:53:31,262 (Thread-2): SQL status: BEGIN in 0.02 seconds
2019-02-07 15:53:31,262 (Thread-2): Using redshift connection "my_first_dbt_model".
2019-02-07 15:53:31,262 (Thread-2): On my_first_dbt_model: drop view if exists "dbt_alex"."my_first_dbt_model__dbt_backup" cascade
2019-02-07 15:53:31,285 (Thread-2): SQL status: DROP VIEW in 0.02 seconds
2019-02-07 15:53:31,286 (Thread-2): On my_first_dbt_model: COMMIT
2019-02-07 15:53:31,286 (Thread-2): Using redshift connection "my_first_dbt_model".
2019-02-07 15:53:31,286 (Thread-2): On my_first_dbt_model: COMMIT
2019-02-07 15:53:31,380 (Thread-2): SQL status: COMMIT in 0.09 seconds
2019-02-07 15:53:31,381 (Thread-2): Using redshift connection "my_first_dbt_model".
2019-02-07 15:53:31,381 (Thread-2): On my_first_dbt_model: BEGIN
2019-02-07 15:53:31,401 (Thread-2): SQL status: BEGIN in 0.02 seconds
2019-02-07 15:53:31,403 (Thread-2): On my_first_dbt_model: ROLLBACK
2019-02-07 15:53:31,424 (Thread-2): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '25a24275-f946-4e25-9b76-9e0b41880ba2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D7C1EB8AC8>]}
2019-02-07 15:53:31,513 (Thread-2): 15:53:31 | 2 of 2 OK created view model dbt_alex.my_first_dbt_model............. [CREATE VIEW in 0.95s]
2019-02-07 15:53:31,584 (MainThread): 15:53:31 | 
2019-02-07 15:53:31,585 (MainThread): 15:53:31 | Finished running 2 view models in 2.22s.
2019-02-07 15:53:31,585 (MainThread): Connection 'master' was left open.
2019-02-07 15:53:31,593 (MainThread): 
2019-02-07 15:53:31,594 (MainThread): Completed with 1 errors:
2019-02-07 15:53:31,595 (MainThread): 
2019-02-07 15:53:31,595 (MainThread): Compilation Error in model util_calendar (models\utils\util_calendar.sql)
2019-02-07 15:53:31,596 (MainThread):   'dbt_utils' is undefined
2019-02-07 15:53:31,596 (MainThread): 
Done. PASS=1 ERROR=1 SKIP=0 TOTAL=2
2019-02-07 15:53:31,596 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D7BF586518>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D7C1E12B38>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D7C1E12080>]}
2019-02-07 15:53:31,694 (MainThread): Flushing usage events
2019-02-07 15:53:50,312 (MainThread): Tracking: tracking
2019-02-07 15:53:50,313 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020421DA2400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020421DA2860>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020421DA2358>]}
2019-02-07 15:53:50,750 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020421D8CB00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020421D8C780>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020421D8C4A8>]}
2019-02-07 15:53:50,853 (MainThread): Flushing usage events
2019-02-07 15:53:50,853 (MainThread): Encountered an error:
2019-02-07 15:53:50,853 (MainThread): 'dict' object has no attribute 'packages'
2019-02-07 15:53:50,855 (MainThread): Traceback (most recent call last):
  File "c:\users\yichy\appdata\local\programs\python\python36\lib\site-packages\dbt\main.py", line 76, in main
    results, succeeded = handle_and_check(args)
  File "c:\users\yichy\appdata\local\programs\python\python36\lib\site-packages\dbt\main.py", line 126, in handle_and_check
    task, res = run_from_args(parsed)
  File "c:\users\yichy\appdata\local\programs\python\python36\lib\site-packages\dbt\main.py", line 181, in run_from_args
    results = run_from_task(task, cfg, parsed)
  File "c:\users\yichy\appdata\local\programs\python\python36\lib\site-packages\dbt\main.py", line 189, in run_from_task
    result = task.run()
  File "c:\users\yichy\appdata\local\programs\python\python36\lib\site-packages\dbt\task\deps.py", line 440, in run
    sub_deps.incorporate_from_yaml(target_config.packages.packages)
AttributeError: 'dict' object has no attribute 'packages'

2019-02-07 15:54:43,672 (MainThread): Tracking: tracking
2019-02-07 15:54:43,672 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021C108D5828>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021C108D5CF8>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021C108D53C8>]}
2019-02-07 15:54:43,834 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021C0ECF64A8>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021C0D124BE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021C0D124C88>]}
2019-02-07 15:54:43,932 (MainThread): Flushing usage events
2019-02-07 15:54:50,193 (MainThread): Tracking: tracking
2019-02-07 15:54:50,194 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000182613B2978>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000182613B26A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000182613B2438>]}
2019-02-07 15:54:50,630 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001826139D898>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001826139D828>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001826139DE48>]}
2019-02-07 15:54:50,728 (MainThread): Flushing usage events
2019-02-07 15:54:50,728 (MainThread): Encountered an error:
2019-02-07 15:54:50,728 (MainThread): 'dict' object has no attribute 'packages'
2019-02-07 15:54:50,730 (MainThread): Traceback (most recent call last):
  File "c:\users\yichy\appdata\local\programs\python\python36\lib\site-packages\dbt\main.py", line 76, in main
    results, succeeded = handle_and_check(args)
  File "c:\users\yichy\appdata\local\programs\python\python36\lib\site-packages\dbt\main.py", line 126, in handle_and_check
    task, res = run_from_args(parsed)
  File "c:\users\yichy\appdata\local\programs\python\python36\lib\site-packages\dbt\main.py", line 181, in run_from_args
    results = run_from_task(task, cfg, parsed)
  File "c:\users\yichy\appdata\local\programs\python\python36\lib\site-packages\dbt\main.py", line 189, in run_from_task
    result = task.run()
  File "c:\users\yichy\appdata\local\programs\python\python36\lib\site-packages\dbt\task\deps.py", line 440, in run
    sub_deps.incorporate_from_yaml(target_config.packages.packages)
AttributeError: 'dict' object has no attribute 'packages'

2019-02-07 15:56:21,722 (MainThread): Tracking: tracking
2019-02-07 15:56:21,722 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018B12A221D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018B12A22780>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018B12A22470>]}
2019-02-07 15:56:25,539 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018B12A17F28>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018B12A17630>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018B12A17358>]}
2019-02-07 15:56:25,631 (MainThread): Flushing usage events
2019-02-07 15:56:25,631 (MainThread): Encountered an error:
2019-02-07 15:56:25,632 (MainThread): 'dict' object has no attribute 'packages'
2019-02-07 15:56:25,633 (MainThread): Traceback (most recent call last):
  File "c:\users\yichy\appdata\local\programs\python\python36\lib\site-packages\dbt\main.py", line 76, in main
    results, succeeded = handle_and_check(args)
  File "c:\users\yichy\appdata\local\programs\python\python36\lib\site-packages\dbt\main.py", line 126, in handle_and_check
    task, res = run_from_args(parsed)
  File "c:\users\yichy\appdata\local\programs\python\python36\lib\site-packages\dbt\main.py", line 181, in run_from_args
    results = run_from_task(task, cfg, parsed)
  File "c:\users\yichy\appdata\local\programs\python\python36\lib\site-packages\dbt\main.py", line 189, in run_from_task
    result = task.run()
  File "c:\users\yichy\appdata\local\programs\python\python36\lib\site-packages\dbt\task\deps.py", line 440, in run
    sub_deps.incorporate_from_yaml(target_config.packages.packages)
AttributeError: 'dict' object has no attribute 'packages'

2019-02-07 15:57:33,210 (MainThread): Tracking: tracking
2019-02-07 15:57:33,211 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000181EC162630>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000181EC1625F8>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000181EC1629E8>]}
2019-02-07 15:57:33,382 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000181EBFB6588>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000181E89C60F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000181E97AB898>]}
2019-02-07 15:57:33,480 (MainThread): Flushing usage events
2019-02-07 15:57:46,517 (MainThread): Tracking: tracking
2019-02-07 15:57:46,517 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BD8CAD5C50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BD8CAD5CF8>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BD8CAD5E10>]}
2019-02-07 15:57:46,991 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BD8CADF390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BD8CADFA20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BD8CADFB70>]}
2019-02-07 15:57:47,088 (MainThread): Flushing usage events
2019-02-07 15:57:47,088 (MainThread): Encountered an error:
2019-02-07 15:57:47,089 (MainThread): 'dict' object has no attribute 'packages'
2019-02-07 15:57:47,090 (MainThread): Traceback (most recent call last):
  File "c:\users\yichy\appdata\local\programs\python\python36\lib\site-packages\dbt\main.py", line 76, in main
    results, succeeded = handle_and_check(args)
  File "c:\users\yichy\appdata\local\programs\python\python36\lib\site-packages\dbt\main.py", line 126, in handle_and_check
    task, res = run_from_args(parsed)
  File "c:\users\yichy\appdata\local\programs\python\python36\lib\site-packages\dbt\main.py", line 181, in run_from_args
    results = run_from_task(task, cfg, parsed)
  File "c:\users\yichy\appdata\local\programs\python\python36\lib\site-packages\dbt\main.py", line 189, in run_from_task
    result = task.run()
  File "c:\users\yichy\appdata\local\programs\python\python36\lib\site-packages\dbt\task\deps.py", line 440, in run
    sub_deps.incorporate_from_yaml(target_config.packages.packages)
AttributeError: 'dict' object has no attribute 'packages'

2019-02-08 09:34:40,538 (MainThread): Tracking: tracking
2019-02-08 09:34:40,539 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D1364F2518>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D1364F26A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D1364F2160>]}
2019-02-08 09:34:40,677 (MainThread): Loading dependency project from c:\users\yichy\appdata\local\programs\python\python36\lib\site-packages\dbt\include
2019-02-08 09:34:40,684 (MainThread): Loading dependency project from C:\users\yichy\dbt-learn\dbt_modules
2019-02-08 09:34:40,693 (MainThread): Parsing macros\core.sql
2019-02-08 09:34:40,700 (MainThread): Parsing macros\adapters\bigquery.sql
2019-02-08 09:34:40,705 (MainThread): Parsing macros\adapters\common.sql
2019-02-08 09:34:40,719 (MainThread): Parsing macros\adapters\redshift.sql
2019-02-08 09:34:40,729 (MainThread): Parsing macros\adapters\snowflake.sql
2019-02-08 09:34:40,732 (MainThread): Parsing macros\catalog\postgres_catalog.sql
2019-02-08 09:34:40,735 (MainThread): Parsing macros\catalog\redshift_catalog.sql
2019-02-08 09:34:40,744 (MainThread): Parsing macros\catalog\snowflake_catalog.sql
2019-02-08 09:34:40,746 (MainThread): Parsing macros\etc\bigquery.sql
2019-02-08 09:34:40,748 (MainThread): Parsing macros\etc\datetime.sql
2019-02-08 09:34:40,758 (MainThread): Parsing macros\etc\get_custom_schema.sql
2019-02-08 09:34:40,761 (MainThread): Parsing macros\materializations\helpers.sql
2019-02-08 09:34:40,771 (MainThread): Parsing macros\materializations\archive\archive.sql
2019-02-08 09:34:40,801 (MainThread): Parsing macros\materializations\common\merge.sql
2019-02-08 09:34:40,806 (MainThread): Parsing macros\materializations\incremental\bigquery_incremental.sql
2019-02-08 09:34:40,811 (MainThread): Parsing macros\materializations\incremental\incremental.sql
2019-02-08 09:34:40,820 (MainThread): Parsing macros\materializations\seed\bigquery.sql
2019-02-08 09:34:40,823 (MainThread): Parsing macros\materializations\seed\seed.sql
2019-02-08 09:34:40,837 (MainThread): Parsing macros\materializations\table\bigquery_table.sql
2019-02-08 09:34:40,846 (MainThread): Parsing macros\materializations\table\snowflake_table.sql
2019-02-08 09:34:40,855 (MainThread): Parsing macros\materializations\table\table.sql
2019-02-08 09:34:40,864 (MainThread): Parsing macros\materializations\view\bq_snowflake_view.sql
2019-02-08 09:34:40,872 (MainThread): Parsing macros\materializations\view\view.sql
2019-02-08 09:34:40,879 (MainThread): Parsing macros\operations\catalog\get_catalog.sql
2019-02-08 09:34:40,880 (MainThread): Parsing macros\operations\relations\get_relations.sql
2019-02-08 09:34:40,882 (MainThread): Parsing macros\relations\postgres_relations.sql
2019-02-08 09:34:40,884 (MainThread): Parsing macros\relations\redshift_relations.sql
2019-02-08 09:34:40,885 (MainThread): Parsing macros\schema_tests\accepted_values.sql
2019-02-08 09:34:40,887 (MainThread): Parsing macros\schema_tests\not_null.sql
2019-02-08 09:34:40,888 (MainThread): Parsing macros\schema_tests\relationships.sql
2019-02-08 09:34:40,890 (MainThread): Parsing macros\schema_tests\unique.sql
2019-02-08 09:34:40,896 (MainThread): Parsing macros\core.sql
2019-02-08 09:34:40,899 (MainThread): Parsing macros\adapters\bigquery.sql
2019-02-08 09:34:40,903 (MainThread): Parsing macros\adapters\common.sql
2019-02-08 09:34:40,910 (MainThread): Parsing macros\adapters\redshift.sql
2019-02-08 09:34:40,916 (MainThread): Parsing macros\adapters\snowflake.sql
2019-02-08 09:34:40,917 (MainThread): Parsing macros\catalog\postgres_catalog.sql
2019-02-08 09:34:40,919 (MainThread): Parsing macros\catalog\redshift_catalog.sql
2019-02-08 09:34:40,925 (MainThread): Parsing macros\catalog\snowflake_catalog.sql
2019-02-08 09:34:40,926 (MainThread): Parsing macros\etc\bigquery.sql
2019-02-08 09:34:40,927 (MainThread): Parsing macros\etc\datetime.sql
2019-02-08 09:34:40,934 (MainThread): Parsing macros\etc\get_custom_schema.sql
2019-02-08 09:34:40,936 (MainThread): Parsing macros\materializations\helpers.sql
2019-02-08 09:34:40,941 (MainThread): Parsing macros\materializations\archive\archive.sql
2019-02-08 09:34:40,961 (MainThread): Parsing macros\materializations\common\merge.sql
2019-02-08 09:34:40,965 (MainThread): Parsing macros\materializations\incremental\bigquery_incremental.sql
2019-02-08 09:34:40,970 (MainThread): Parsing macros\materializations\incremental\incremental.sql
2019-02-08 09:34:40,979 (MainThread): Parsing macros\materializations\seed\bigquery.sql
2019-02-08 09:34:40,981 (MainThread): Parsing macros\materializations\seed\seed.sql
2019-02-08 09:34:40,993 (MainThread): Parsing macros\materializations\table\bigquery_table.sql
2019-02-08 09:34:41,000 (MainThread): Parsing macros\materializations\table\snowflake_table.sql
2019-02-08 09:34:41,009 (MainThread): Parsing macros\materializations\table\table.sql
2019-02-08 09:34:41,016 (MainThread): Parsing macros\materializations\view\bq_snowflake_view.sql
2019-02-08 09:34:41,023 (MainThread): Parsing macros\materializations\view\view.sql
2019-02-08 09:34:41,030 (MainThread): Parsing macros\operations\catalog\get_catalog.sql
2019-02-08 09:34:41,031 (MainThread): Parsing macros\operations\relations\get_relations.sql
2019-02-08 09:34:41,032 (MainThread): Parsing macros\relations\postgres_relations.sql
2019-02-08 09:34:41,034 (MainThread): Parsing macros\relations\redshift_relations.sql
2019-02-08 09:34:41,035 (MainThread): Parsing macros\schema_tests\accepted_values.sql
2019-02-08 09:34:41,036 (MainThread): Parsing macros\schema_tests\not_null.sql
2019-02-08 09:34:41,037 (MainThread): Parsing macros\schema_tests\relationships.sql
2019-02-08 09:34:41,039 (MainThread): Parsing macros\schema_tests\unique.sql
2019-02-08 09:34:41,040 (MainThread): Parsing model.dbt_learn.property_type_by_zipcode
2019-02-08 09:34:41,076 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.my_new_package.example

2019-02-08 09:34:41,081 (MainThread): Found 1 models, 0 tests, 0 archives, 0 analyses, 94 macros, 2 operations, 0 seed files
2019-02-08 09:34:41,083 (MainThread): 
2019-02-08 09:34:41,083 (MainThread): 09:34:41 | Concurrency: 4 threads (target='dev')
2019-02-08 09:34:41,084 (MainThread): 09:34:41 | 
2019-02-08 09:34:41,087 (Thread-1): Compiling model.dbt_learn.property_type_by_zipcode
2019-02-08 09:34:41,091 (Thread-1): Writing injected SQL for node "model.dbt_learn.property_type_by_zipcode"
2019-02-08 09:34:41,192 (MainThread): 09:34:41 | Done.
2019-02-08 09:34:41,193 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D133C67518>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D132DF3908>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D1364ECDA0>]}
2019-02-08 09:34:41,292 (MainThread): Flushing usage events
2019-02-08 09:36:47,828 (MainThread): Tracking: tracking
2019-02-08 09:36:47,828 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE65B02048>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE65B02390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE65B02710>]}
2019-02-08 09:36:47,962 (MainThread): Loading dependency project from c:\users\yichy\appdata\local\programs\python\python36\lib\site-packages\dbt\include
2019-02-08 09:36:47,969 (MainThread): Loading dependency project from C:\users\yichy\dbt-learn\dbt_modules
2019-02-08 09:36:47,975 (MainThread): Parsing macros\core.sql
2019-02-08 09:36:47,980 (MainThread): Parsing macros\adapters\bigquery.sql
2019-02-08 09:36:47,985 (MainThread): Parsing macros\adapters\common.sql
2019-02-08 09:36:47,997 (MainThread): Parsing macros\adapters\redshift.sql
2019-02-08 09:36:48,004 (MainThread): Parsing macros\adapters\snowflake.sql
2019-02-08 09:36:48,006 (MainThread): Parsing macros\catalog\postgres_catalog.sql
2019-02-08 09:36:48,008 (MainThread): Parsing macros\catalog\redshift_catalog.sql
2019-02-08 09:36:48,016 (MainThread): Parsing macros\catalog\snowflake_catalog.sql
2019-02-08 09:36:48,018 (MainThread): Parsing macros\etc\bigquery.sql
2019-02-08 09:36:48,020 (MainThread): Parsing macros\etc\datetime.sql
2019-02-08 09:36:48,028 (MainThread): Parsing macros\etc\get_custom_schema.sql
2019-02-08 09:36:48,031 (MainThread): Parsing macros\materializations\helpers.sql
2019-02-08 09:36:48,038 (MainThread): Parsing macros\materializations\archive\archive.sql
2019-02-08 09:36:48,062 (MainThread): Parsing macros\materializations\common\merge.sql
2019-02-08 09:36:48,067 (MainThread): Parsing macros\materializations\incremental\bigquery_incremental.sql
2019-02-08 09:36:48,073 (MainThread): Parsing macros\materializations\incremental\incremental.sql
2019-02-08 09:36:48,083 (MainThread): Parsing macros\materializations\seed\bigquery.sql
2019-02-08 09:36:48,086 (MainThread): Parsing macros\materializations\seed\seed.sql
2019-02-08 09:36:48,100 (MainThread): Parsing macros\materializations\table\bigquery_table.sql
2019-02-08 09:36:48,109 (MainThread): Parsing macros\materializations\table\snowflake_table.sql
2019-02-08 09:36:48,118 (MainThread): Parsing macros\materializations\table\table.sql
2019-02-08 09:36:48,126 (MainThread): Parsing macros\materializations\view\bq_snowflake_view.sql
2019-02-08 09:36:48,136 (MainThread): Parsing macros\materializations\view\view.sql
2019-02-08 09:36:48,143 (MainThread): Parsing macros\operations\catalog\get_catalog.sql
2019-02-08 09:36:48,144 (MainThread): Parsing macros\operations\relations\get_relations.sql
2019-02-08 09:36:48,146 (MainThread): Parsing macros\relations\postgres_relations.sql
2019-02-08 09:36:48,148 (MainThread): Parsing macros\relations\redshift_relations.sql
2019-02-08 09:36:48,149 (MainThread): Parsing macros\schema_tests\accepted_values.sql
2019-02-08 09:36:48,151 (MainThread): Parsing macros\schema_tests\not_null.sql
2019-02-08 09:36:48,153 (MainThread): Parsing macros\schema_tests\relationships.sql
2019-02-08 09:36:48,154 (MainThread): Parsing macros\schema_tests\unique.sql
2019-02-08 09:36:48,162 (MainThread): Parsing macros\core.sql
2019-02-08 09:36:48,166 (MainThread): Parsing macros\adapters\bigquery.sql
2019-02-08 09:36:48,169 (MainThread): Parsing macros\adapters\common.sql
2019-02-08 09:36:48,179 (MainThread): Parsing macros\adapters\redshift.sql
2019-02-08 09:36:48,185 (MainThread): Parsing macros\adapters\snowflake.sql
2019-02-08 09:36:48,187 (MainThread): Parsing macros\catalog\postgres_catalog.sql
2019-02-08 09:36:48,188 (MainThread): Parsing macros\catalog\redshift_catalog.sql
2019-02-08 09:36:48,194 (MainThread): Parsing macros\catalog\snowflake_catalog.sql
2019-02-08 09:36:48,196 (MainThread): Parsing macros\etc\bigquery.sql
2019-02-08 09:36:48,197 (MainThread): Parsing macros\etc\datetime.sql
2019-02-08 09:36:48,204 (MainThread): Parsing macros\etc\get_custom_schema.sql
2019-02-08 09:36:48,206 (MainThread): Parsing macros\materializations\helpers.sql
2019-02-08 09:36:48,211 (MainThread): Parsing macros\materializations\archive\archive.sql
2019-02-08 09:36:48,232 (MainThread): Parsing macros\materializations\common\merge.sql
2019-02-08 09:36:48,236 (MainThread): Parsing macros\materializations\incremental\bigquery_incremental.sql
2019-02-08 09:36:48,241 (MainThread): Parsing macros\materializations\incremental\incremental.sql
2019-02-08 09:36:48,250 (MainThread): Parsing macros\materializations\seed\bigquery.sql
2019-02-08 09:36:48,252 (MainThread): Parsing macros\materializations\seed\seed.sql
2019-02-08 09:36:48,266 (MainThread): Parsing macros\materializations\table\bigquery_table.sql
2019-02-08 09:36:48,274 (MainThread): Parsing macros\materializations\table\snowflake_table.sql
2019-02-08 09:36:48,282 (MainThread): Parsing macros\materializations\table\table.sql
2019-02-08 09:36:48,291 (MainThread): Parsing macros\materializations\view\bq_snowflake_view.sql
2019-02-08 09:36:48,299 (MainThread): Parsing macros\materializations\view\view.sql
2019-02-08 09:36:48,305 (MainThread): Parsing macros\operations\catalog\get_catalog.sql
2019-02-08 09:36:48,307 (MainThread): Parsing macros\operations\relations\get_relations.sql
2019-02-08 09:36:48,308 (MainThread): Parsing macros\relations\postgres_relations.sql
2019-02-08 09:36:48,310 (MainThread): Parsing macros\relations\redshift_relations.sql
2019-02-08 09:36:48,310 (MainThread): Parsing macros\schema_tests\accepted_values.sql
2019-02-08 09:36:48,312 (MainThread): Parsing macros\schema_tests\not_null.sql
2019-02-08 09:36:48,313 (MainThread): Parsing macros\schema_tests\relationships.sql
2019-02-08 09:36:48,315 (MainThread): Parsing macros\schema_tests\unique.sql
2019-02-08 09:36:48,317 (MainThread): Parsing model.dbt_learn.property_type_by_zipcode
2019-02-08 09:36:48,357 (MainThread): Found 1 models, 0 tests, 0 archives, 0 analyses, 94 macros, 2 operations, 0 seed files
2019-02-08 09:36:48,359 (MainThread): 
2019-02-08 09:36:48,359 (MainThread): 09:36:48 | Concurrency: 4 threads (target='dev')
2019-02-08 09:36:48,359 (MainThread): 09:36:48 | 
2019-02-08 09:36:48,363 (Thread-1): Compiling model.dbt_learn.property_type_by_zipcode
2019-02-08 09:36:48,368 (Thread-1): Writing injected SQL for node "model.dbt_learn.property_type_by_zipcode"
2019-02-08 09:36:48,466 (MainThread): 09:36:48 | Done.
2019-02-08 09:36:48,467 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE6234F320>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE6234F278>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE6314C898>]}
2019-02-08 09:36:48,554 (MainThread): Flushing usage events
2019-02-08 09:40:58,323 (MainThread): Tracking: tracking
2019-02-08 09:40:58,323 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000206D5D50AC8>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000206D5D50CC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000206D5D50828>]}
2019-02-08 09:40:58,495 (MainThread): Loading dependency project from c:\users\yichy\appdata\local\programs\python\python36\lib\site-packages\dbt\include
2019-02-08 09:40:58,500 (MainThread): Loading dependency project from C:\users\yichy\dbt-learn\dbt_modules
2019-02-08 09:40:58,506 (MainThread): Parsing macros\core.sql
2019-02-08 09:40:58,511 (MainThread): Parsing macros\adapters\bigquery.sql
2019-02-08 09:40:58,516 (MainThread): Parsing macros\adapters\common.sql
2019-02-08 09:40:58,527 (MainThread): Parsing macros\adapters\redshift.sql
2019-02-08 09:40:58,533 (MainThread): Parsing macros\adapters\snowflake.sql
2019-02-08 09:40:58,535 (MainThread): Parsing macros\catalog\postgres_catalog.sql
2019-02-08 09:40:58,537 (MainThread): Parsing macros\catalog\redshift_catalog.sql
2019-02-08 09:40:58,543 (MainThread): Parsing macros\catalog\snowflake_catalog.sql
2019-02-08 09:40:58,545 (MainThread): Parsing macros\etc\bigquery.sql
2019-02-08 09:40:58,545 (MainThread): Parsing macros\etc\datetime.sql
2019-02-08 09:40:58,557 (MainThread): Parsing macros\etc\get_custom_schema.sql
2019-02-08 09:40:58,559 (MainThread): Parsing macros\materializations\helpers.sql
2019-02-08 09:40:58,566 (MainThread): Parsing macros\materializations\archive\archive.sql
2019-02-08 09:40:58,588 (MainThread): Parsing macros\materializations\common\merge.sql
2019-02-08 09:40:58,593 (MainThread): Parsing macros\materializations\incremental\bigquery_incremental.sql
2019-02-08 09:40:58,598 (MainThread): Parsing macros\materializations\incremental\incremental.sql
2019-02-08 09:40:58,607 (MainThread): Parsing macros\materializations\seed\bigquery.sql
2019-02-08 09:40:58,610 (MainThread): Parsing macros\materializations\seed\seed.sql
2019-02-08 09:40:58,624 (MainThread): Parsing macros\materializations\table\bigquery_table.sql
2019-02-08 09:40:58,632 (MainThread): Parsing macros\materializations\table\snowflake_table.sql
2019-02-08 09:40:58,642 (MainThread): Parsing macros\materializations\table\table.sql
2019-02-08 09:40:58,650 (MainThread): Parsing macros\materializations\view\bq_snowflake_view.sql
2019-02-08 09:40:58,659 (MainThread): Parsing macros\materializations\view\view.sql
2019-02-08 09:40:58,665 (MainThread): Parsing macros\operations\catalog\get_catalog.sql
2019-02-08 09:40:58,666 (MainThread): Parsing macros\operations\relations\get_relations.sql
2019-02-08 09:40:58,667 (MainThread): Parsing macros\relations\postgres_relations.sql
2019-02-08 09:40:58,669 (MainThread): Parsing macros\relations\redshift_relations.sql
2019-02-08 09:40:58,670 (MainThread): Parsing macros\schema_tests\accepted_values.sql
2019-02-08 09:40:58,672 (MainThread): Parsing macros\schema_tests\not_null.sql
2019-02-08 09:40:58,674 (MainThread): Parsing macros\schema_tests\relationships.sql
2019-02-08 09:40:58,675 (MainThread): Parsing macros\schema_tests\unique.sql
2019-02-08 09:40:58,682 (MainThread): Parsing macros\core.sql
2019-02-08 09:40:58,685 (MainThread): Parsing macros\adapters\bigquery.sql
2019-02-08 09:40:58,688 (MainThread): Parsing macros\adapters\common.sql
2019-02-08 09:40:58,697 (MainThread): Parsing macros\adapters\redshift.sql
2019-02-08 09:40:58,702 (MainThread): Parsing macros\adapters\snowflake.sql
2019-02-08 09:40:58,704 (MainThread): Parsing macros\catalog\postgres_catalog.sql
2019-02-08 09:40:58,705 (MainThread): Parsing macros\catalog\redshift_catalog.sql
2019-02-08 09:40:58,710 (MainThread): Parsing macros\catalog\snowflake_catalog.sql
2019-02-08 09:40:58,712 (MainThread): Parsing macros\etc\bigquery.sql
2019-02-08 09:40:58,713 (MainThread): Parsing macros\etc\datetime.sql
2019-02-08 09:40:58,719 (MainThread): Parsing macros\etc\get_custom_schema.sql
2019-02-08 09:40:58,721 (MainThread): Parsing macros\materializations\helpers.sql
2019-02-08 09:40:58,726 (MainThread): Parsing macros\materializations\archive\archive.sql
2019-02-08 09:40:58,745 (MainThread): Parsing macros\materializations\common\merge.sql
2019-02-08 09:40:58,749 (MainThread): Parsing macros\materializations\incremental\bigquery_incremental.sql
2019-02-08 09:40:58,754 (MainThread): Parsing macros\materializations\incremental\incremental.sql
2019-02-08 09:40:58,762 (MainThread): Parsing macros\materializations\seed\bigquery.sql
2019-02-08 09:40:58,764 (MainThread): Parsing macros\materializations\seed\seed.sql
2019-02-08 09:40:58,776 (MainThread): Parsing macros\materializations\table\bigquery_table.sql
2019-02-08 09:40:58,783 (MainThread): Parsing macros\materializations\table\snowflake_table.sql
2019-02-08 09:40:58,792 (MainThread): Parsing macros\materializations\table\table.sql
2019-02-08 09:40:58,799 (MainThread): Parsing macros\materializations\view\bq_snowflake_view.sql
2019-02-08 09:40:58,806 (MainThread): Parsing macros\materializations\view\view.sql
2019-02-08 09:40:58,813 (MainThread): Parsing macros\operations\catalog\get_catalog.sql
2019-02-08 09:40:58,814 (MainThread): Parsing macros\operations\relations\get_relations.sql
2019-02-08 09:40:58,815 (MainThread): Parsing macros\relations\postgres_relations.sql
2019-02-08 09:40:58,817 (MainThread): Parsing macros\relations\redshift_relations.sql
2019-02-08 09:40:58,818 (MainThread): Parsing macros\schema_tests\accepted_values.sql
2019-02-08 09:40:58,819 (MainThread): Parsing macros\schema_tests\not_null.sql
2019-02-08 09:40:58,821 (MainThread): Parsing macros\schema_tests\relationships.sql
2019-02-08 09:40:58,822 (MainThread): Parsing macros\schema_tests\unique.sql
2019-02-08 09:40:58,823 (MainThread): Parsing model.dbt_learn.property_type_by_zipcode
2019-02-08 09:40:58,861 (MainThread): Found 1 models, 0 tests, 0 archives, 0 analyses, 94 macros, 2 operations, 0 seed files
2019-02-08 09:40:58,863 (MainThread): 
2019-02-08 09:40:58,863 (MainThread): Acquiring new redshift connection "master".
2019-02-08 09:40:58,863 (MainThread): Opening a new connection (0 currently allocated)
2019-02-08 09:40:58,864 (MainThread): Connecting to Redshift using 'database' credentials
2019-02-08 09:40:59,640 (MainThread): Using redshift connection "master".
2019-02-08 09:40:59,640 (MainThread): On master: select tablename as name, schemaname as schema, 'table' as type from pg_tables
        where schemaname ilike 'dbt_alex'
        union all
        select viewname as name, schemaname as schema, 'view' as type from pg_views
        where schemaname ilike 'dbt_alex'
2019-02-08 09:40:59,687 (MainThread): SQL status: SELECT in 0.05 seconds
2019-02-08 09:40:59,721 (MainThread): Acquiring new redshift connection "get_relations_data".
2019-02-08 09:40:59,721 (MainThread): Opening a new connection (1 currently allocated)
2019-02-08 09:40:59,722 (MainThread): Connecting to Redshift using 'database' credentials
2019-02-08 09:40:59,877 (MainThread): Using redshift connection "get_relations_data".
2019-02-08 09:40:59,877 (MainThread): On get_relations_data: BEGIN
2019-02-08 09:40:59,918 (MainThread): SQL status: BEGIN in 0.04 seconds
2019-02-08 09:40:59,918 (MainThread): Using redshift connection "get_relations_data".
2019-02-08 09:40:59,918 (MainThread): On get_relations_data: -- 
    with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg_%'
    ),
    relationships as (
        select
            referenced_class.name as referenced_name,
            referenced_class.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced_class.kind as kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        join dependency on relation.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            referenced_class.kind in ('r', 'v') and
            (referenced_class.name != dependent_class.name or
             referenced_class.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2019-02-08 09:40:59,966 (MainThread): SQL status: SELECT in 0.05 seconds
2019-02-08 09:40:59,973 (MainThread): On get_relations_data: ROLLBACK
2019-02-08 09:40:59,992 (MainThread): Using redshift connection "master".
2019-02-08 09:40:59,992 (MainThread): On master: select distinct nspname from pg_namespace
2019-02-08 09:41:00,008 (MainThread): SQL status: SELECT in 0.02 seconds
2019-02-08 09:41:00,009 (MainThread): 09:41:00 | Concurrency: 4 threads (target='dev')
2019-02-08 09:41:00,009 (MainThread): 09:41:00 | 
2019-02-08 09:41:00,013 (Thread-1): 09:41:00 | 1 of 1 START view model dbt_alex.property_type_by_zipcode............ [RUN]
2019-02-08 09:41:00,015 (Thread-1): Compiling model.dbt_learn.property_type_by_zipcode
2019-02-08 09:41:00,026 (Thread-1): Writing injected SQL for node "model.dbt_learn.property_type_by_zipcode"
2019-02-08 09:41:00,078 (Thread-1): Acquiring new redshift connection "property_type_by_zipcode".
2019-02-08 09:41:00,079 (Thread-1): Re-using an available connection from the pool.
2019-02-08 09:41:00,079 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:41:00,079 (Thread-1): On property_type_by_zipcode: BEGIN
2019-02-08 09:41:00,118 (Thread-1): SQL status: BEGIN in 0.04 seconds
2019-02-08 09:41:00,118 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:41:00,119 (Thread-1): On property_type_by_zipcode: drop view if exists "dbt_alex"."property_type_by_zipcode__dbt_tmp" cascade
2019-02-08 09:41:00,139 (Thread-1): SQL status: DROP VIEW in 0.02 seconds
2019-02-08 09:41:00,140 (Thread-1): On property_type_by_zipcode: COMMIT
2019-02-08 09:41:00,140 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:41:00,140 (Thread-1): On property_type_by_zipcode: COMMIT
2019-02-08 09:41:00,234 (Thread-1): SQL status: COMMIT in 0.09 seconds
2019-02-08 09:41:00,235 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:41:00,235 (Thread-1): On property_type_by_zipcode: BEGIN
2019-02-08 09:41:00,255 (Thread-1): SQL status: BEGIN in 0.02 seconds
2019-02-08 09:41:00,255 (Thread-1): On property_type_by_zipcode: COMMIT
2019-02-08 09:41:00,255 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:41:00,255 (Thread-1): On property_type_by_zipcode: COMMIT
2019-02-08 09:41:00,274 (Thread-1): SQL status: COMMIT in 0.02 seconds
2019-02-08 09:41:00,275 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:41:00,275 (Thread-1): On property_type_by_zipcode: BEGIN
2019-02-08 09:41:00,294 (Thread-1): SQL status: BEGIN in 0.02 seconds
2019-02-08 09:41:00,295 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:41:00,295 (Thread-1): On property_type_by_zipcode: drop view if exists "dbt_alex"."property_type_by_zipcode__dbt_backup" cascade
2019-02-08 09:41:00,317 (Thread-1): SQL status: DROP VIEW in 0.02 seconds
2019-02-08 09:41:00,317 (Thread-1): On property_type_by_zipcode: COMMIT
2019-02-08 09:41:00,317 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:41:00,317 (Thread-1): On property_type_by_zipcode: COMMIT
2019-02-08 09:41:00,755 (Thread-1): SQL status: COMMIT in 0.44 seconds
2019-02-08 09:41:00,756 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:41:00,756 (Thread-1): On property_type_by_zipcode: BEGIN
2019-02-08 09:41:00,775 (Thread-1): SQL status: BEGIN in 0.02 seconds
2019-02-08 09:41:00,788 (Thread-1): Writing runtime SQL for node "model.dbt_learn.property_type_by_zipcode"
2019-02-08 09:41:00,792 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:41:00,792 (Thread-1): On property_type_by_zipcode: 

  create view "dbt_alex"."property_type_by_zipcode__dbt_tmp" as (
    select
    zipcode,
    
    -- writing sql using jinja instead of repeating the case when clause
    sum(case when lower(property_type) = 'house' then 1 else 0 end) as house_count
    
    -- writing sql using jinja instead of repeating the case when clause
    sum(case when lower(property_type) = 'apartment' then 1 else 0 end) as apartment_count
    
    -- writing sql using jinja instead of repeating the case when clause
    sum(case when lower(property_type) = 'townhouse' then 1 else 0 end) as townhouse_count
    

from source_data.listings

group by 1

limit 100
  ) ;
2019-02-08 09:41:00,812 (Thread-1): Postgres error: syntax error at or near "sum"
LINE 11:     sum(case when lower(property_type) = 'apartment' then 1 ...
             ^

2019-02-08 09:41:00,813 (Thread-1): On property_type_by_zipcode: ROLLBACK
2019-02-08 09:41:00,834 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '85cd9209-b082-44bb-8be9-3c07ccca8a67', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000206D5D159E8>]}
2019-02-08 09:41:00,923 (Thread-1): 09:41:00 | 1 of 1 ERROR creating view model dbt_alex.property_type_by_zipcode... [ERROR in 0.82s]
2019-02-08 09:41:01,018 (MainThread): 09:41:01 | 
2019-02-08 09:41:01,019 (MainThread): 09:41:01 | Finished running 1 view models in 2.16s.
2019-02-08 09:41:01,020 (MainThread): Connection 'master' was left open.
2019-02-08 09:41:01,024 (MainThread): 
2019-02-08 09:41:01,024 (MainThread): Completed with 1 errors:
2019-02-08 09:41:01,025 (MainThread): 
2019-02-08 09:41:01,026 (MainThread): Database Error in model property_type_by_zipcode (models\property_type_by_zipcode.sql)
2019-02-08 09:41:01,027 (MainThread):   syntax error at or near "sum"
2019-02-08 09:41:01,028 (MainThread):   LINE 11:     sum(case when lower(property_type) = 'apartment' then 1 ...
2019-02-08 09:41:01,029 (MainThread):                ^
2019-02-08 09:41:01,031 (MainThread):   compiled SQL at target\compiled\dbt_learn\property_type_by_zipcode.sql
2019-02-08 09:41:01,032 (MainThread): 
Done. PASS=0 ERROR=1 SKIP=0 TOTAL=1
2019-02-08 09:41:01,033 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000206D34993C8>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000206D4132518>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000206D5D1F240>]}
2019-02-08 09:41:01,135 (MainThread): Flushing usage events
2019-02-08 09:44:43,816 (MainThread): Tracking: tracking
2019-02-08 09:44:43,816 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025EE5DB0B38>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025EE5DB0D30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025EE5DB0898>]}
2019-02-08 09:44:43,997 (MainThread): Loading dependency project from c:\users\yichy\appdata\local\programs\python\python36\lib\site-packages\dbt\include
2019-02-08 09:44:44,004 (MainThread): Loading dependency project from C:\users\yichy\dbt-learn\dbt_modules
2019-02-08 09:44:44,011 (MainThread): Parsing macros\core.sql
2019-02-08 09:44:44,017 (MainThread): Parsing macros\adapters\bigquery.sql
2019-02-08 09:44:44,022 (MainThread): Parsing macros\adapters\common.sql
2019-02-08 09:44:44,035 (MainThread): Parsing macros\adapters\redshift.sql
2019-02-08 09:44:44,043 (MainThread): Parsing macros\adapters\snowflake.sql
2019-02-08 09:44:44,046 (MainThread): Parsing macros\catalog\postgres_catalog.sql
2019-02-08 09:44:44,048 (MainThread): Parsing macros\catalog\redshift_catalog.sql
2019-02-08 09:44:44,056 (MainThread): Parsing macros\catalog\snowflake_catalog.sql
2019-02-08 09:44:44,058 (MainThread): Parsing macros\etc\bigquery.sql
2019-02-08 09:44:44,059 (MainThread): Parsing macros\etc\datetime.sql
2019-02-08 09:44:44,069 (MainThread): Parsing macros\etc\get_custom_schema.sql
2019-02-08 09:44:44,072 (MainThread): Parsing macros\materializations\helpers.sql
2019-02-08 09:44:44,080 (MainThread): Parsing macros\materializations\archive\archive.sql
2019-02-08 09:44:44,107 (MainThread): Parsing macros\materializations\common\merge.sql
2019-02-08 09:44:44,112 (MainThread): Parsing macros\materializations\incremental\bigquery_incremental.sql
2019-02-08 09:44:44,119 (MainThread): Parsing macros\materializations\incremental\incremental.sql
2019-02-08 09:44:44,130 (MainThread): Parsing macros\materializations\seed\bigquery.sql
2019-02-08 09:44:44,134 (MainThread): Parsing macros\materializations\seed\seed.sql
2019-02-08 09:44:44,151 (MainThread): Parsing macros\materializations\table\bigquery_table.sql
2019-02-08 09:44:44,160 (MainThread): Parsing macros\materializations\table\snowflake_table.sql
2019-02-08 09:44:44,171 (MainThread): Parsing macros\materializations\table\table.sql
2019-02-08 09:44:44,180 (MainThread): Parsing macros\materializations\view\bq_snowflake_view.sql
2019-02-08 09:44:44,191 (MainThread): Parsing macros\materializations\view\view.sql
2019-02-08 09:44:44,199 (MainThread): Parsing macros\operations\catalog\get_catalog.sql
2019-02-08 09:44:44,201 (MainThread): Parsing macros\operations\relations\get_relations.sql
2019-02-08 09:44:44,202 (MainThread): Parsing macros\relations\postgres_relations.sql
2019-02-08 09:44:44,203 (MainThread): Parsing macros\relations\redshift_relations.sql
2019-02-08 09:44:44,205 (MainThread): Parsing macros\schema_tests\accepted_values.sql
2019-02-08 09:44:44,206 (MainThread): Parsing macros\schema_tests\not_null.sql
2019-02-08 09:44:44,208 (MainThread): Parsing macros\schema_tests\relationships.sql
2019-02-08 09:44:44,209 (MainThread): Parsing macros\schema_tests\unique.sql
2019-02-08 09:44:44,216 (MainThread): Parsing macros\core.sql
2019-02-08 09:44:44,221 (MainThread): Parsing macros\adapters\bigquery.sql
2019-02-08 09:44:44,225 (MainThread): Parsing macros\adapters\common.sql
2019-02-08 09:44:44,234 (MainThread): Parsing macros\adapters\redshift.sql
2019-02-08 09:44:44,241 (MainThread): Parsing macros\adapters\snowflake.sql
2019-02-08 09:44:44,243 (MainThread): Parsing macros\catalog\postgres_catalog.sql
2019-02-08 09:44:44,244 (MainThread): Parsing macros\catalog\redshift_catalog.sql
2019-02-08 09:44:44,251 (MainThread): Parsing macros\catalog\snowflake_catalog.sql
2019-02-08 09:44:44,253 (MainThread): Parsing macros\etc\bigquery.sql
2019-02-08 09:44:44,254 (MainThread): Parsing macros\etc\datetime.sql
2019-02-08 09:44:44,262 (MainThread): Parsing macros\etc\get_custom_schema.sql
2019-02-08 09:44:44,264 (MainThread): Parsing macros\materializations\helpers.sql
2019-02-08 09:44:44,270 (MainThread): Parsing macros\materializations\archive\archive.sql
2019-02-08 09:44:44,294 (MainThread): Parsing macros\materializations\common\merge.sql
2019-02-08 09:44:44,298 (MainThread): Parsing macros\materializations\incremental\bigquery_incremental.sql
2019-02-08 09:44:44,303 (MainThread): Parsing macros\materializations\incremental\incremental.sql
2019-02-08 09:44:44,313 (MainThread): Parsing macros\materializations\seed\bigquery.sql
2019-02-08 09:44:44,315 (MainThread): Parsing macros\materializations\seed\seed.sql
2019-02-08 09:44:44,329 (MainThread): Parsing macros\materializations\table\bigquery_table.sql
2019-02-08 09:44:44,338 (MainThread): Parsing macros\materializations\table\snowflake_table.sql
2019-02-08 09:44:44,347 (MainThread): Parsing macros\materializations\table\table.sql
2019-02-08 09:44:44,356 (MainThread): Parsing macros\materializations\view\bq_snowflake_view.sql
2019-02-08 09:44:44,364 (MainThread): Parsing macros\materializations\view\view.sql
2019-02-08 09:44:44,372 (MainThread): Parsing macros\operations\catalog\get_catalog.sql
2019-02-08 09:44:44,373 (MainThread): Parsing macros\operations\relations\get_relations.sql
2019-02-08 09:44:44,375 (MainThread): Parsing macros\relations\postgres_relations.sql
2019-02-08 09:44:44,377 (MainThread): Parsing macros\relations\redshift_relations.sql
2019-02-08 09:44:44,378 (MainThread): Parsing macros\schema_tests\accepted_values.sql
2019-02-08 09:44:44,380 (MainThread): Parsing macros\schema_tests\not_null.sql
2019-02-08 09:44:44,381 (MainThread): Parsing macros\schema_tests\relationships.sql
2019-02-08 09:44:44,382 (MainThread): Parsing macros\schema_tests\unique.sql
2019-02-08 09:44:44,384 (MainThread): Parsing model.dbt_learn.property_type_by_zipcode
2019-02-08 09:44:44,426 (MainThread): Found 1 models, 0 tests, 0 archives, 0 analyses, 94 macros, 2 operations, 0 seed files
2019-02-08 09:44:44,428 (MainThread): 
2019-02-08 09:44:44,428 (MainThread): Acquiring new redshift connection "master".
2019-02-08 09:44:44,428 (MainThread): Opening a new connection (0 currently allocated)
2019-02-08 09:44:44,429 (MainThread): Connecting to Redshift using 'database' credentials
2019-02-08 09:44:45,181 (MainThread): Using redshift connection "master".
2019-02-08 09:44:45,181 (MainThread): On master: select tablename as name, schemaname as schema, 'table' as type from pg_tables
        where schemaname ilike 'dbt_alex'
        union all
        select viewname as name, schemaname as schema, 'view' as type from pg_views
        where schemaname ilike 'dbt_alex'
2019-02-08 09:44:45,229 (MainThread): SQL status: SELECT in 0.05 seconds
2019-02-08 09:44:45,269 (MainThread): Acquiring new redshift connection "get_relations_data".
2019-02-08 09:44:45,269 (MainThread): Opening a new connection (1 currently allocated)
2019-02-08 09:44:45,270 (MainThread): Connecting to Redshift using 'database' credentials
2019-02-08 09:44:45,432 (MainThread): Using redshift connection "get_relations_data".
2019-02-08 09:44:45,432 (MainThread): On get_relations_data: BEGIN
2019-02-08 09:44:45,475 (MainThread): SQL status: BEGIN in 0.04 seconds
2019-02-08 09:44:45,475 (MainThread): Using redshift connection "get_relations_data".
2019-02-08 09:44:45,476 (MainThread): On get_relations_data: -- 
    with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg_%'
    ),
    relationships as (
        select
            referenced_class.name as referenced_name,
            referenced_class.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced_class.kind as kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        join dependency on relation.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            referenced_class.kind in ('r', 'v') and
            (referenced_class.name != dependent_class.name or
             referenced_class.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2019-02-08 09:44:45,521 (MainThread): SQL status: SELECT in 0.05 seconds
2019-02-08 09:44:45,530 (MainThread): On get_relations_data: ROLLBACK
2019-02-08 09:44:45,551 (MainThread): Using redshift connection "master".
2019-02-08 09:44:45,551 (MainThread): On master: select distinct nspname from pg_namespace
2019-02-08 09:44:45,569 (MainThread): SQL status: SELECT in 0.02 seconds
2019-02-08 09:44:45,569 (MainThread): 09:44:45 | Concurrency: 4 threads (target='dev')
2019-02-08 09:44:45,570 (MainThread): 09:44:45 | 
2019-02-08 09:44:45,574 (Thread-1): 09:44:45 | 1 of 1 START view model dbt_alex.property_type_by_zipcode............ [RUN]
2019-02-08 09:44:45,575 (Thread-1): Compiling model.dbt_learn.property_type_by_zipcode
2019-02-08 09:44:45,580 (Thread-1): Writing injected SQL for node "model.dbt_learn.property_type_by_zipcode"
2019-02-08 09:44:45,628 (Thread-1): Acquiring new redshift connection "property_type_by_zipcode".
2019-02-08 09:44:45,628 (Thread-1): Re-using an available connection from the pool.
2019-02-08 09:44:45,629 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:44:45,629 (Thread-1): On property_type_by_zipcode: BEGIN
2019-02-08 09:44:45,669 (Thread-1): SQL status: BEGIN in 0.04 seconds
2019-02-08 09:44:45,669 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:44:45,670 (Thread-1): On property_type_by_zipcode: drop view if exists "dbt_alex"."property_type_by_zipcode__dbt_tmp" cascade
2019-02-08 09:44:45,691 (Thread-1): SQL status: DROP VIEW in 0.02 seconds
2019-02-08 09:44:45,691 (Thread-1): On property_type_by_zipcode: COMMIT
2019-02-08 09:44:45,691 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:44:45,691 (Thread-1): On property_type_by_zipcode: COMMIT
2019-02-08 09:44:45,779 (Thread-1): SQL status: COMMIT in 0.09 seconds
2019-02-08 09:44:45,780 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:44:45,780 (Thread-1): On property_type_by_zipcode: BEGIN
2019-02-08 09:44:45,803 (Thread-1): SQL status: BEGIN in 0.02 seconds
2019-02-08 09:44:45,804 (Thread-1): On property_type_by_zipcode: COMMIT
2019-02-08 09:44:45,804 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:44:45,804 (Thread-1): On property_type_by_zipcode: COMMIT
2019-02-08 09:44:45,822 (Thread-1): SQL status: COMMIT in 0.02 seconds
2019-02-08 09:44:45,823 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:44:45,823 (Thread-1): On property_type_by_zipcode: BEGIN
2019-02-08 09:44:45,844 (Thread-1): SQL status: BEGIN in 0.02 seconds
2019-02-08 09:44:45,844 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:44:45,845 (Thread-1): On property_type_by_zipcode: drop view if exists "dbt_alex"."property_type_by_zipcode__dbt_backup" cascade
2019-02-08 09:44:45,865 (Thread-1): SQL status: DROP VIEW in 0.02 seconds
2019-02-08 09:44:45,865 (Thread-1): On property_type_by_zipcode: COMMIT
2019-02-08 09:44:45,866 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:44:45,866 (Thread-1): On property_type_by_zipcode: COMMIT
2019-02-08 09:44:46,031 (Thread-1): SQL status: COMMIT in 0.17 seconds
2019-02-08 09:44:46,032 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:44:46,032 (Thread-1): On property_type_by_zipcode: BEGIN
2019-02-08 09:44:46,053 (Thread-1): SQL status: BEGIN in 0.02 seconds
2019-02-08 09:44:46,068 (Thread-1): Writing runtime SQL for node "model.dbt_learn.property_type_by_zipcode"
2019-02-08 09:44:46,070 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:44:46,071 (Thread-1): On property_type_by_zipcode: 

  create view "dbt_alex"."property_type_by_zipcode__dbt_tmp" as (
    select
    zipcode
    
    -- writing sql using jinja instead of repeating the case when clause
    ,sum(case when lower(property_type) = 'house' then 1 else 0 end) as house_count

    
    ,
    

    
    -- writing sql using jinja instead of repeating the case when clause
    ,sum(case when lower(property_type) = 'apartment' then 1 else 0 end) as apartment_count

    
    ,
    

    
    -- writing sql using jinja instead of repeating the case when clause
    ,sum(case when lower(property_type) = 'townhouse' then 1 else 0 end) as townhouse_count

    

    

from source_data.listings

group by 1

limit 100
  ) ;
2019-02-08 09:44:46,091 (Thread-1): Postgres error: syntax error at or near ","
LINE 16:     ,sum(case when lower(property_type) = 'apartment' then 1...
             ^

2019-02-08 09:44:46,091 (Thread-1): On property_type_by_zipcode: ROLLBACK
2019-02-08 09:44:46,112 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '940bf284-ada1-4e1b-965f-28f4ca8b3dd9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025EE5D75A90>]}
2019-02-08 09:44:46,267 (Thread-1): 09:44:46 | 1 of 1 ERROR creating view model dbt_alex.property_type_by_zipcode... [ERROR in 0.54s]
2019-02-08 09:44:46,279 (MainThread): 09:44:46 | 
2019-02-08 09:44:46,279 (MainThread): 09:44:46 | Finished running 1 view models in 1.85s.
2019-02-08 09:44:46,280 (MainThread): Connection 'master' was left open.
2019-02-08 09:44:46,285 (MainThread): 
2019-02-08 09:44:46,285 (MainThread): Completed with 1 errors:
2019-02-08 09:44:46,287 (MainThread): 
2019-02-08 09:44:46,287 (MainThread): Database Error in model property_type_by_zipcode (models\property_type_by_zipcode.sql)
2019-02-08 09:44:46,289 (MainThread):   syntax error at or near ","
2019-02-08 09:44:46,290 (MainThread):   LINE 16:     ,sum(case when lower(property_type) = 'apartment' then 1...
2019-02-08 09:44:46,291 (MainThread):                ^
2019-02-08 09:44:46,291 (MainThread):   compiled SQL at target\compiled\dbt_learn\property_type_by_zipcode.sql
2019-02-08 09:44:46,292 (MainThread): 
Done. PASS=0 ERROR=1 SKIP=0 TOTAL=1
2019-02-08 09:44:46,293 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025EE5DB3710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025EE4193518>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025EE34F93C8>]}
2019-02-08 09:44:46,388 (MainThread): Flushing usage events
2019-02-08 09:47:33,369 (MainThread): Tracking: tracking
2019-02-08 09:47:33,369 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C7494E0AC8>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C7494E0CC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C7494E0828>]}
2019-02-08 09:47:33,553 (MainThread): Loading dependency project from c:\users\yichy\appdata\local\programs\python\python36\lib\site-packages\dbt\include
2019-02-08 09:47:33,559 (MainThread): Loading dependency project from C:\users\yichy\dbt-learn\dbt_modules
2019-02-08 09:47:33,565 (MainThread): Parsing macros\core.sql
2019-02-08 09:47:33,570 (MainThread): Parsing macros\adapters\bigquery.sql
2019-02-08 09:47:33,575 (MainThread): Parsing macros\adapters\common.sql
2019-02-08 09:47:33,587 (MainThread): Parsing macros\adapters\redshift.sql
2019-02-08 09:47:33,594 (MainThread): Parsing macros\adapters\snowflake.sql
2019-02-08 09:47:33,596 (MainThread): Parsing macros\catalog\postgres_catalog.sql
2019-02-08 09:47:33,598 (MainThread): Parsing macros\catalog\redshift_catalog.sql
2019-02-08 09:47:33,605 (MainThread): Parsing macros\catalog\snowflake_catalog.sql
2019-02-08 09:47:33,607 (MainThread): Parsing macros\etc\bigquery.sql
2019-02-08 09:47:33,608 (MainThread): Parsing macros\etc\datetime.sql
2019-02-08 09:47:33,617 (MainThread): Parsing macros\etc\get_custom_schema.sql
2019-02-08 09:47:33,619 (MainThread): Parsing macros\materializations\helpers.sql
2019-02-08 09:47:33,627 (MainThread): Parsing macros\materializations\archive\archive.sql
2019-02-08 09:47:33,653 (MainThread): Parsing macros\materializations\common\merge.sql
2019-02-08 09:47:33,657 (MainThread): Parsing macros\materializations\incremental\bigquery_incremental.sql
2019-02-08 09:47:33,663 (MainThread): Parsing macros\materializations\incremental\incremental.sql
2019-02-08 09:47:33,673 (MainThread): Parsing macros\materializations\seed\bigquery.sql
2019-02-08 09:47:33,676 (MainThread): Parsing macros\materializations\seed\seed.sql
2019-02-08 09:47:33,691 (MainThread): Parsing macros\materializations\table\bigquery_table.sql
2019-02-08 09:47:33,700 (MainThread): Parsing macros\materializations\table\snowflake_table.sql
2019-02-08 09:47:33,709 (MainThread): Parsing macros\materializations\table\table.sql
2019-02-08 09:47:33,717 (MainThread): Parsing macros\materializations\view\bq_snowflake_view.sql
2019-02-08 09:47:33,725 (MainThread): Parsing macros\materializations\view\view.sql
2019-02-08 09:47:33,733 (MainThread): Parsing macros\operations\catalog\get_catalog.sql
2019-02-08 09:47:33,734 (MainThread): Parsing macros\operations\relations\get_relations.sql
2019-02-08 09:47:33,735 (MainThread): Parsing macros\relations\postgres_relations.sql
2019-02-08 09:47:33,737 (MainThread): Parsing macros\relations\redshift_relations.sql
2019-02-08 09:47:33,738 (MainThread): Parsing macros\schema_tests\accepted_values.sql
2019-02-08 09:47:33,740 (MainThread): Parsing macros\schema_tests\not_null.sql
2019-02-08 09:47:33,741 (MainThread): Parsing macros\schema_tests\relationships.sql
2019-02-08 09:47:33,743 (MainThread): Parsing macros\schema_tests\unique.sql
2019-02-08 09:47:33,750 (MainThread): Parsing macros\core.sql
2019-02-08 09:47:33,753 (MainThread): Parsing macros\adapters\bigquery.sql
2019-02-08 09:47:33,757 (MainThread): Parsing macros\adapters\common.sql
2019-02-08 09:47:33,765 (MainThread): Parsing macros\adapters\redshift.sql
2019-02-08 09:47:33,771 (MainThread): Parsing macros\adapters\snowflake.sql
2019-02-08 09:47:33,772 (MainThread): Parsing macros\catalog\postgres_catalog.sql
2019-02-08 09:47:33,774 (MainThread): Parsing macros\catalog\redshift_catalog.sql
2019-02-08 09:47:33,780 (MainThread): Parsing macros\catalog\snowflake_catalog.sql
2019-02-08 09:47:33,781 (MainThread): Parsing macros\etc\bigquery.sql
2019-02-08 09:47:33,782 (MainThread): Parsing macros\etc\datetime.sql
2019-02-08 09:47:33,789 (MainThread): Parsing macros\etc\get_custom_schema.sql
2019-02-08 09:47:33,792 (MainThread): Parsing macros\materializations\helpers.sql
2019-02-08 09:47:33,797 (MainThread): Parsing macros\materializations\archive\archive.sql
2019-02-08 09:47:33,818 (MainThread): Parsing macros\materializations\common\merge.sql
2019-02-08 09:47:33,821 (MainThread): Parsing macros\materializations\incremental\bigquery_incremental.sql
2019-02-08 09:47:33,827 (MainThread): Parsing macros\materializations\incremental\incremental.sql
2019-02-08 09:47:33,836 (MainThread): Parsing macros\materializations\seed\bigquery.sql
2019-02-08 09:47:33,838 (MainThread): Parsing macros\materializations\seed\seed.sql
2019-02-08 09:47:33,851 (MainThread): Parsing macros\materializations\table\bigquery_table.sql
2019-02-08 09:47:33,859 (MainThread): Parsing macros\materializations\table\snowflake_table.sql
2019-02-08 09:47:33,867 (MainThread): Parsing macros\materializations\table\table.sql
2019-02-08 09:47:33,874 (MainThread): Parsing macros\materializations\view\bq_snowflake_view.sql
2019-02-08 09:47:33,881 (MainThread): Parsing macros\materializations\view\view.sql
2019-02-08 09:47:33,888 (MainThread): Parsing macros\operations\catalog\get_catalog.sql
2019-02-08 09:47:33,890 (MainThread): Parsing macros\operations\relations\get_relations.sql
2019-02-08 09:47:33,891 (MainThread): Parsing macros\relations\postgres_relations.sql
2019-02-08 09:47:33,893 (MainThread): Parsing macros\relations\redshift_relations.sql
2019-02-08 09:47:33,894 (MainThread): Parsing macros\schema_tests\accepted_values.sql
2019-02-08 09:47:33,895 (MainThread): Parsing macros\schema_tests\not_null.sql
2019-02-08 09:47:33,896 (MainThread): Parsing macros\schema_tests\relationships.sql
2019-02-08 09:47:33,898 (MainThread): Parsing macros\schema_tests\unique.sql
2019-02-08 09:47:33,900 (MainThread): Parsing model.dbt_learn.property_type_by_zipcode
2019-02-08 09:47:33,940 (MainThread): Found 1 models, 0 tests, 0 archives, 0 analyses, 94 macros, 2 operations, 0 seed files
2019-02-08 09:47:33,942 (MainThread): 
2019-02-08 09:47:33,942 (MainThread): Acquiring new redshift connection "master".
2019-02-08 09:47:33,942 (MainThread): Opening a new connection (0 currently allocated)
2019-02-08 09:47:33,943 (MainThread): Connecting to Redshift using 'database' credentials
2019-02-08 09:47:34,838 (MainThread): Using redshift connection "master".
2019-02-08 09:47:34,838 (MainThread): On master: select tablename as name, schemaname as schema, 'table' as type from pg_tables
        where schemaname ilike 'dbt_alex'
        union all
        select viewname as name, schemaname as schema, 'view' as type from pg_views
        where schemaname ilike 'dbt_alex'
2019-02-08 09:47:34,894 (MainThread): SQL status: SELECT in 0.06 seconds
2019-02-08 09:47:34,930 (MainThread): Acquiring new redshift connection "get_relations_data".
2019-02-08 09:47:34,930 (MainThread): Opening a new connection (1 currently allocated)
2019-02-08 09:47:34,931 (MainThread): Connecting to Redshift using 'database' credentials
2019-02-08 09:47:35,080 (MainThread): Using redshift connection "get_relations_data".
2019-02-08 09:47:35,080 (MainThread): On get_relations_data: BEGIN
2019-02-08 09:47:35,121 (MainThread): SQL status: BEGIN in 0.04 seconds
2019-02-08 09:47:35,122 (MainThread): Using redshift connection "get_relations_data".
2019-02-08 09:47:35,122 (MainThread): On get_relations_data: -- 
    with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg_%'
    ),
    relationships as (
        select
            referenced_class.name as referenced_name,
            referenced_class.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced_class.kind as kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        join dependency on relation.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            referenced_class.kind in ('r', 'v') and
            (referenced_class.name != dependent_class.name or
             referenced_class.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2019-02-08 09:47:35,171 (MainThread): SQL status: SELECT in 0.05 seconds
2019-02-08 09:47:35,179 (MainThread): On get_relations_data: ROLLBACK
2019-02-08 09:47:35,200 (MainThread): Using redshift connection "master".
2019-02-08 09:47:35,200 (MainThread): On master: select distinct nspname from pg_namespace
2019-02-08 09:47:35,223 (MainThread): SQL status: SELECT in 0.02 seconds
2019-02-08 09:47:35,223 (MainThread): 09:47:35 | Concurrency: 4 threads (target='dev')
2019-02-08 09:47:35,223 (MainThread): 09:47:35 | 
2019-02-08 09:47:35,226 (Thread-1): 09:47:35 | 1 of 1 START view model dbt_alex.property_type_by_zipcode............ [RUN]
2019-02-08 09:47:35,228 (Thread-1): Compiling model.dbt_learn.property_type_by_zipcode
2019-02-08 09:47:35,233 (Thread-1): Writing injected SQL for node "model.dbt_learn.property_type_by_zipcode"
2019-02-08 09:47:35,273 (Thread-1): Acquiring new redshift connection "property_type_by_zipcode".
2019-02-08 09:47:35,273 (Thread-1): Re-using an available connection from the pool.
2019-02-08 09:47:35,274 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:47:35,274 (Thread-1): On property_type_by_zipcode: BEGIN
2019-02-08 09:47:35,313 (Thread-1): SQL status: BEGIN in 0.04 seconds
2019-02-08 09:47:35,314 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:47:35,314 (Thread-1): On property_type_by_zipcode: drop view if exists "dbt_alex"."property_type_by_zipcode__dbt_tmp" cascade
2019-02-08 09:47:35,335 (Thread-1): SQL status: DROP VIEW in 0.02 seconds
2019-02-08 09:47:35,335 (Thread-1): On property_type_by_zipcode: COMMIT
2019-02-08 09:47:35,335 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:47:35,335 (Thread-1): On property_type_by_zipcode: COMMIT
2019-02-08 09:47:35,431 (Thread-1): SQL status: COMMIT in 0.10 seconds
2019-02-08 09:47:35,431 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:47:35,431 (Thread-1): On property_type_by_zipcode: BEGIN
2019-02-08 09:47:35,451 (Thread-1): SQL status: BEGIN in 0.02 seconds
2019-02-08 09:47:35,451 (Thread-1): On property_type_by_zipcode: COMMIT
2019-02-08 09:47:35,452 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:47:35,452 (Thread-1): On property_type_by_zipcode: COMMIT
2019-02-08 09:47:35,471 (Thread-1): SQL status: COMMIT in 0.02 seconds
2019-02-08 09:47:35,472 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:47:35,472 (Thread-1): On property_type_by_zipcode: BEGIN
2019-02-08 09:47:35,495 (Thread-1): SQL status: BEGIN in 0.02 seconds
2019-02-08 09:47:35,495 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:47:35,496 (Thread-1): On property_type_by_zipcode: drop view if exists "dbt_alex"."property_type_by_zipcode__dbt_backup" cascade
2019-02-08 09:47:35,517 (Thread-1): SQL status: DROP VIEW in 0.02 seconds
2019-02-08 09:47:35,517 (Thread-1): On property_type_by_zipcode: COMMIT
2019-02-08 09:47:35,517 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:47:35,517 (Thread-1): On property_type_by_zipcode: COMMIT
2019-02-08 09:47:35,603 (Thread-1): SQL status: COMMIT in 0.09 seconds
2019-02-08 09:47:35,604 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:47:35,604 (Thread-1): On property_type_by_zipcode: BEGIN
2019-02-08 09:47:35,625 (Thread-1): SQL status: BEGIN in 0.02 seconds
2019-02-08 09:47:35,640 (Thread-1): Writing runtime SQL for node "model.dbt_learn.property_type_by_zipcode"
2019-02-08 09:47:35,643 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:47:35,643 (Thread-1): On property_type_by_zipcode: 

  create view "dbt_alex"."property_type_by_zipcode__dbt_tmp" as (
    select
    zipcode
    
    -- writing sql using jinja instead of repeating the case when clause
    sum(case when lower(property_type) = 'house' then 1 else 0 end) as house_count,
    -- writing sql using jinja instead of repeating the case when clause
    sum(case when lower(property_type) = 'apartment' then 1 else 0 end) as apartment_count,
    -- writing sql using jinja instead of repeating the case when clause
    sum(case when lower(property_type) = 'townhouse' then 1 else 0 end) as townhouse_count

from source_data.listings

group by 1

limit 100
  ) ;
2019-02-08 09:47:35,663 (Thread-1): Postgres error: syntax error at or near "("
LINE 8:     sum(case when lower(property_type) = 'house' then 1 else...
               ^

2019-02-08 09:47:35,663 (Thread-1): On property_type_by_zipcode: ROLLBACK
2019-02-08 09:47:35,689 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '93871e2b-89d1-45e0-a54d-3739bb045450', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C749564C50>]}
2019-02-08 09:47:35,787 (Thread-1): 09:47:35 | 1 of 1 ERROR creating view model dbt_alex.property_type_by_zipcode... [ERROR in 0.46s]
2019-02-08 09:47:35,836 (MainThread): 09:47:35 | 
2019-02-08 09:47:35,836 (MainThread): 09:47:35 | Finished running 1 view models in 1.89s.
2019-02-08 09:47:35,837 (MainThread): Connection 'master' was left open.
2019-02-08 09:47:35,843 (MainThread): 
2019-02-08 09:47:35,843 (MainThread): Completed with 1 errors:
2019-02-08 09:47:35,844 (MainThread): 
2019-02-08 09:47:35,845 (MainThread): Database Error in model property_type_by_zipcode (models\property_type_by_zipcode.sql)
2019-02-08 09:47:35,848 (MainThread):   syntax error at or near "("
2019-02-08 09:47:35,848 (MainThread):   LINE 8:     sum(case when lower(property_type) = 'house' then 1 else...
2019-02-08 09:47:35,849 (MainThread):                  ^
2019-02-08 09:47:35,850 (MainThread):   compiled SQL at target\compiled\dbt_learn\property_type_by_zipcode.sql
2019-02-08 09:47:35,850 (MainThread): 
Done. PASS=0 ERROR=1 SKIP=0 TOTAL=1
2019-02-08 09:47:35,851 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C749491DD8>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C746AF9898>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C746C293C8>]}
2019-02-08 09:47:35,944 (MainThread): Flushing usage events
2019-02-08 09:49:44,784 (MainThread): Tracking: tracking
2019-02-08 09:49:44,784 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FD747520F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FD747524E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FD74752128>]}
2019-02-08 09:49:44,955 (MainThread): Loading dependency project from c:\users\yichy\appdata\local\programs\python\python36\lib\site-packages\dbt\include
2019-02-08 09:49:44,968 (MainThread): Loading dependency project from C:\users\yichy\dbt-learn\dbt_modules
2019-02-08 09:49:44,975 (MainThread): Parsing macros\core.sql
2019-02-08 09:49:44,982 (MainThread): Parsing macros\adapters\bigquery.sql
2019-02-08 09:49:44,987 (MainThread): Parsing macros\adapters\common.sql
2019-02-08 09:49:45,000 (MainThread): Parsing macros\adapters\redshift.sql
2019-02-08 09:49:45,007 (MainThread): Parsing macros\adapters\snowflake.sql
2019-02-08 09:49:45,009 (MainThread): Parsing macros\catalog\postgres_catalog.sql
2019-02-08 09:49:45,011 (MainThread): Parsing macros\catalog\redshift_catalog.sql
2019-02-08 09:49:45,018 (MainThread): Parsing macros\catalog\snowflake_catalog.sql
2019-02-08 09:49:45,020 (MainThread): Parsing macros\etc\bigquery.sql
2019-02-08 09:49:45,021 (MainThread): Parsing macros\etc\datetime.sql
2019-02-08 09:49:45,029 (MainThread): Parsing macros\etc\get_custom_schema.sql
2019-02-08 09:49:45,031 (MainThread): Parsing macros\materializations\helpers.sql
2019-02-08 09:49:45,038 (MainThread): Parsing macros\materializations\archive\archive.sql
2019-02-08 09:49:45,070 (MainThread): Parsing macros\materializations\common\merge.sql
2019-02-08 09:49:45,075 (MainThread): Parsing macros\materializations\incremental\bigquery_incremental.sql
2019-02-08 09:49:45,081 (MainThread): Parsing macros\materializations\incremental\incremental.sql
2019-02-08 09:49:45,091 (MainThread): Parsing macros\materializations\seed\bigquery.sql
2019-02-08 09:49:45,094 (MainThread): Parsing macros\materializations\seed\seed.sql
2019-02-08 09:49:45,116 (MainThread): Parsing macros\materializations\table\bigquery_table.sql
2019-02-08 09:49:45,124 (MainThread): Parsing macros\materializations\table\snowflake_table.sql
2019-02-08 09:49:45,133 (MainThread): Parsing macros\materializations\table\table.sql
2019-02-08 09:49:45,141 (MainThread): Parsing macros\materializations\view\bq_snowflake_view.sql
2019-02-08 09:49:45,149 (MainThread): Parsing macros\materializations\view\view.sql
2019-02-08 09:49:45,156 (MainThread): Parsing macros\operations\catalog\get_catalog.sql
2019-02-08 09:49:45,157 (MainThread): Parsing macros\operations\relations\get_relations.sql
2019-02-08 09:49:45,158 (MainThread): Parsing macros\relations\postgres_relations.sql
2019-02-08 09:49:45,160 (MainThread): Parsing macros\relations\redshift_relations.sql
2019-02-08 09:49:45,161 (MainThread): Parsing macros\schema_tests\accepted_values.sql
2019-02-08 09:49:45,163 (MainThread): Parsing macros\schema_tests\not_null.sql
2019-02-08 09:49:45,164 (MainThread): Parsing macros\schema_tests\relationships.sql
2019-02-08 09:49:45,166 (MainThread): Parsing macros\schema_tests\unique.sql
2019-02-08 09:49:45,173 (MainThread): Parsing macros\core.sql
2019-02-08 09:49:45,177 (MainThread): Parsing macros\adapters\bigquery.sql
2019-02-08 09:49:45,180 (MainThread): Parsing macros\adapters\common.sql
2019-02-08 09:49:45,194 (MainThread): Parsing macros\adapters\redshift.sql
2019-02-08 09:49:45,200 (MainThread): Parsing macros\adapters\snowflake.sql
2019-02-08 09:49:45,201 (MainThread): Parsing macros\catalog\postgres_catalog.sql
2019-02-08 09:49:45,203 (MainThread): Parsing macros\catalog\redshift_catalog.sql
2019-02-08 09:49:45,208 (MainThread): Parsing macros\catalog\snowflake_catalog.sql
2019-02-08 09:49:45,210 (MainThread): Parsing macros\etc\bigquery.sql
2019-02-08 09:49:45,211 (MainThread): Parsing macros\etc\datetime.sql
2019-02-08 09:49:45,217 (MainThread): Parsing macros\etc\get_custom_schema.sql
2019-02-08 09:49:45,219 (MainThread): Parsing macros\materializations\helpers.sql
2019-02-08 09:49:45,224 (MainThread): Parsing macros\materializations\archive\archive.sql
2019-02-08 09:49:45,244 (MainThread): Parsing macros\materializations\common\merge.sql
2019-02-08 09:49:45,247 (MainThread): Parsing macros\materializations\incremental\bigquery_incremental.sql
2019-02-08 09:49:45,253 (MainThread): Parsing macros\materializations\incremental\incremental.sql
2019-02-08 09:49:45,261 (MainThread): Parsing macros\materializations\seed\bigquery.sql
2019-02-08 09:49:45,263 (MainThread): Parsing macros\materializations\seed\seed.sql
2019-02-08 09:49:45,275 (MainThread): Parsing macros\materializations\table\bigquery_table.sql
2019-02-08 09:49:45,283 (MainThread): Parsing macros\materializations\table\snowflake_table.sql
2019-02-08 09:49:45,291 (MainThread): Parsing macros\materializations\table\table.sql
2019-02-08 09:49:45,299 (MainThread): Parsing macros\materializations\view\bq_snowflake_view.sql
2019-02-08 09:49:45,306 (MainThread): Parsing macros\materializations\view\view.sql
2019-02-08 09:49:45,312 (MainThread): Parsing macros\operations\catalog\get_catalog.sql
2019-02-08 09:49:45,313 (MainThread): Parsing macros\operations\relations\get_relations.sql
2019-02-08 09:49:45,314 (MainThread): Parsing macros\relations\postgres_relations.sql
2019-02-08 09:49:45,316 (MainThread): Parsing macros\relations\redshift_relations.sql
2019-02-08 09:49:45,317 (MainThread): Parsing macros\schema_tests\accepted_values.sql
2019-02-08 09:49:45,318 (MainThread): Parsing macros\schema_tests\not_null.sql
2019-02-08 09:49:45,319 (MainThread): Parsing macros\schema_tests\relationships.sql
2019-02-08 09:49:45,321 (MainThread): Parsing macros\schema_tests\unique.sql
2019-02-08 09:49:45,322 (MainThread): Parsing model.dbt_learn.property_type_by_zipcode
2019-02-08 09:49:45,338 (MainThread): Parsing model.dbt_learn.util_calendar
2019-02-08 09:49:45,376 (MainThread): Found 2 models, 0 tests, 0 archives, 0 analyses, 94 macros, 2 operations, 0 seed files
2019-02-08 09:49:47,557 (MainThread): 
2019-02-08 09:49:47,558 (MainThread): Acquiring new redshift connection "master".
2019-02-08 09:49:47,558 (MainThread): Opening a new connection (0 currently allocated)
2019-02-08 09:49:47,559 (MainThread): Connecting to Redshift using 'database' credentials
2019-02-08 09:49:48,229 (MainThread): Using redshift connection "master".
2019-02-08 09:49:48,229 (MainThread): On master: select tablename as name, schemaname as schema, 'table' as type from pg_tables
        where schemaname ilike 'dbt_alex'
        union all
        select viewname as name, schemaname as schema, 'view' as type from pg_views
        where schemaname ilike 'dbt_alex'
2019-02-08 09:49:48,280 (MainThread): SQL status: SELECT in 0.05 seconds
2019-02-08 09:49:48,313 (MainThread): Acquiring new redshift connection "get_relations_data".
2019-02-08 09:49:48,314 (MainThread): Opening a new connection (1 currently allocated)
2019-02-08 09:49:48,314 (MainThread): Connecting to Redshift using 'database' credentials
2019-02-08 09:49:48,457 (MainThread): Using redshift connection "get_relations_data".
2019-02-08 09:49:48,457 (MainThread): On get_relations_data: BEGIN
2019-02-08 09:49:48,495 (MainThread): SQL status: BEGIN in 0.04 seconds
2019-02-08 09:49:48,495 (MainThread): Using redshift connection "get_relations_data".
2019-02-08 09:49:48,495 (MainThread): On get_relations_data: -- 
    with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg_%'
    ),
    relationships as (
        select
            referenced_class.name as referenced_name,
            referenced_class.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced_class.kind as kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        join dependency on relation.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            referenced_class.kind in ('r', 'v') and
            (referenced_class.name != dependent_class.name or
             referenced_class.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2019-02-08 09:49:48,542 (MainThread): SQL status: SELECT in 0.05 seconds
2019-02-08 09:49:48,550 (MainThread): On get_relations_data: ROLLBACK
2019-02-08 09:49:48,571 (MainThread): Using redshift connection "master".
2019-02-08 09:49:48,571 (MainThread): On master: select distinct nspname from pg_namespace
2019-02-08 09:49:48,590 (MainThread): SQL status: SELECT in 0.02 seconds
2019-02-08 09:49:48,591 (MainThread): 09:49:48 | Concurrency: 4 threads (target='dev')
2019-02-08 09:49:48,591 (MainThread): 09:49:48 | 
2019-02-08 09:49:48,595 (Thread-1): 09:49:48 | 1 of 2 START view model dbt_alex.property_type_by_zipcode............ [RUN]
2019-02-08 09:49:48,597 (Thread-1): Compiling model.dbt_learn.property_type_by_zipcode
2019-02-08 09:49:48,601 (Thread-1): Writing injected SQL for node "model.dbt_learn.property_type_by_zipcode"
2019-02-08 09:49:48,596 (Thread-2): 09:49:48 | 2 of 2 START view model dbt_alex.util_calendar....................... [RUN]
2019-02-08 09:49:48,602 (Thread-2): Compiling model.dbt_learn.util_calendar
2019-02-08 09:49:48,640 (Thread-2): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1c7ab33e-4962-4bed-a304-b6765b9fcce6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FD7474C3C8>]}
2019-02-08 09:49:48,641 (Thread-1): Acquiring new redshift connection "property_type_by_zipcode".
2019-02-08 09:49:48,641 (Thread-1): Re-using an available connection from the pool.
2019-02-08 09:49:48,643 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:49:48,643 (Thread-1): On property_type_by_zipcode: BEGIN
2019-02-08 09:49:48,683 (Thread-1): SQL status: BEGIN in 0.04 seconds
2019-02-08 09:49:48,684 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:49:48,684 (Thread-1): On property_type_by_zipcode: drop view if exists "dbt_alex"."property_type_by_zipcode__dbt_tmp" cascade
2019-02-08 09:49:48,703 (Thread-1): SQL status: DROP VIEW in 0.02 seconds
2019-02-08 09:49:48,703 (Thread-1): On property_type_by_zipcode: COMMIT
2019-02-08 09:49:48,703 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:49:48,703 (Thread-1): On property_type_by_zipcode: COMMIT
2019-02-08 09:49:48,736 (Thread-2): 09:49:48 | 2 of 2 ERROR creating view model dbt_alex.util_calendar.............. [ERROR in 0.04s]
2019-02-08 09:49:48,789 (Thread-1): SQL status: COMMIT in 0.09 seconds
2019-02-08 09:49:48,789 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:49:48,789 (Thread-1): On property_type_by_zipcode: BEGIN
2019-02-08 09:49:48,808 (Thread-1): SQL status: BEGIN in 0.02 seconds
2019-02-08 09:49:48,808 (Thread-1): On property_type_by_zipcode: COMMIT
2019-02-08 09:49:48,808 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:49:48,808 (Thread-1): On property_type_by_zipcode: COMMIT
2019-02-08 09:49:48,828 (Thread-1): SQL status: COMMIT in 0.02 seconds
2019-02-08 09:49:48,829 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:49:48,829 (Thread-1): On property_type_by_zipcode: BEGIN
2019-02-08 09:49:48,851 (Thread-1): SQL status: BEGIN in 0.02 seconds
2019-02-08 09:49:48,851 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:49:48,851 (Thread-1): On property_type_by_zipcode: drop view if exists "dbt_alex"."property_type_by_zipcode__dbt_backup" cascade
2019-02-08 09:49:48,872 (Thread-1): SQL status: DROP VIEW in 0.02 seconds
2019-02-08 09:49:48,873 (Thread-1): On property_type_by_zipcode: COMMIT
2019-02-08 09:49:48,873 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:49:48,873 (Thread-1): On property_type_by_zipcode: COMMIT
2019-02-08 09:49:48,956 (Thread-1): SQL status: COMMIT in 0.08 seconds
2019-02-08 09:49:48,957 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:49:48,957 (Thread-1): On property_type_by_zipcode: BEGIN
2019-02-08 09:49:48,977 (Thread-1): SQL status: BEGIN in 0.02 seconds
2019-02-08 09:49:48,991 (Thread-1): Writing runtime SQL for node "model.dbt_learn.property_type_by_zipcode"
2019-02-08 09:49:48,993 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:49:48,994 (Thread-1): On property_type_by_zipcode: 

  create view "dbt_alex"."property_type_by_zipcode__dbt_tmp" as (
    select
    zipcode
    
    -- writing sql using jinja instead of repeating the case when clause
    sum(case when lower(property_type) = 'house' then 1 else 0 end) as house_count,
    -- writing sql using jinja instead of repeating the case when clause
    sum(case when lower(property_type) = 'apartment' then 1 else 0 end) as apartment_count,
    -- writing sql using jinja instead of repeating the case when clause
    sum(case when lower(property_type) = 'townhouse' then 1 else 0 end) as townhouse_count

from source_data.listings

group by 1

limit 100
  ) ;
2019-02-08 09:49:49,015 (Thread-1): Postgres error: syntax error at or near "("
LINE 8:     sum(case when lower(property_type) = 'house' then 1 else...
               ^

2019-02-08 09:49:49,016 (Thread-1): On property_type_by_zipcode: ROLLBACK
2019-02-08 09:49:49,043 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1c7ab33e-4962-4bed-a304-b6765b9fcce6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FD7474C400>]}
2019-02-08 09:49:49,128 (Thread-1): 09:49:49 | 1 of 2 ERROR creating view model dbt_alex.property_type_by_zipcode... [ERROR in 0.44s]
2019-02-08 09:49:49,200 (MainThread): 09:49:49 | 
2019-02-08 09:49:49,200 (MainThread): 09:49:49 | Finished running 2 view models in 1.64s.
2019-02-08 09:49:49,201 (MainThread): Connection 'master' was left open.
2019-02-08 09:49:49,206 (MainThread): 
2019-02-08 09:49:49,207 (MainThread): Completed with 2 errors:
2019-02-08 09:49:49,207 (MainThread): 
2019-02-08 09:49:49,208 (MainThread): Compilation Error in model util_calendar (models\utils\util_calendar.sql)
2019-02-08 09:49:49,210 (MainThread):   'dbt_utils' is undefined
2019-02-08 09:49:49,211 (MainThread): 
2019-02-08 09:49:49,212 (MainThread): Database Error in model property_type_by_zipcode (models\property_type_by_zipcode.sql)
2019-02-08 09:49:49,213 (MainThread):   syntax error at or near "("
2019-02-08 09:49:49,214 (MainThread):   LINE 8:     sum(case when lower(property_type) = 'house' then 1 else...
2019-02-08 09:49:49,214 (MainThread):                  ^
2019-02-08 09:49:49,215 (MainThread):   compiled SQL at target\compiled\dbt_learn\property_type_by_zipcode.sql
2019-02-08 09:49:49,215 (MainThread): 
Done. PASS=0 ERROR=2 SKIP=0 TOTAL=2
2019-02-08 09:49:49,216 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FD70F94A58>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FD71D9B898>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FD74752E80>]}
2019-02-08 09:49:49,310 (MainThread): Flushing usage events
2019-02-08 09:50:38,530 (MainThread): Tracking: tracking
2019-02-08 09:50:38,531 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000010DA5D6D208>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000010DA5D6D7F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000010DA5D6DCF8>]}
2019-02-08 09:50:38,765 (MainThread): Loading dependency project from c:\users\yichy\appdata\local\programs\python\python36\lib\site-packages\dbt\include
2019-02-08 09:50:38,771 (MainThread): Loading dependency project from C:\users\yichy\dbt-learn\dbt_modules
2019-02-08 09:50:38,776 (MainThread): Parsing macros\core.sql
2019-02-08 09:50:38,781 (MainThread): Parsing macros\adapters\bigquery.sql
2019-02-08 09:50:38,785 (MainThread): Parsing macros\adapters\common.sql
2019-02-08 09:50:38,796 (MainThread): Parsing macros\adapters\redshift.sql
2019-02-08 09:50:38,803 (MainThread): Parsing macros\adapters\snowflake.sql
2019-02-08 09:50:38,805 (MainThread): Parsing macros\catalog\postgres_catalog.sql
2019-02-08 09:50:38,806 (MainThread): Parsing macros\catalog\redshift_catalog.sql
2019-02-08 09:50:38,813 (MainThread): Parsing macros\catalog\snowflake_catalog.sql
2019-02-08 09:50:38,815 (MainThread): Parsing macros\etc\bigquery.sql
2019-02-08 09:50:38,816 (MainThread): Parsing macros\etc\datetime.sql
2019-02-08 09:50:38,823 (MainThread): Parsing macros\etc\get_custom_schema.sql
2019-02-08 09:50:38,825 (MainThread): Parsing macros\materializations\helpers.sql
2019-02-08 09:50:38,832 (MainThread): Parsing macros\materializations\archive\archive.sql
2019-02-08 09:50:38,855 (MainThread): Parsing macros\materializations\common\merge.sql
2019-02-08 09:50:38,858 (MainThread): Parsing macros\materializations\incremental\bigquery_incremental.sql
2019-02-08 09:50:38,864 (MainThread): Parsing macros\materializations\incremental\incremental.sql
2019-02-08 09:50:38,873 (MainThread): Parsing macros\materializations\seed\bigquery.sql
2019-02-08 09:50:38,876 (MainThread): Parsing macros\materializations\seed\seed.sql
2019-02-08 09:50:38,890 (MainThread): Parsing macros\materializations\table\bigquery_table.sql
2019-02-08 09:50:38,898 (MainThread): Parsing macros\materializations\table\snowflake_table.sql
2019-02-08 09:50:38,906 (MainThread): Parsing macros\materializations\table\table.sql
2019-02-08 09:50:38,913 (MainThread): Parsing macros\materializations\view\bq_snowflake_view.sql
2019-02-08 09:50:38,921 (MainThread): Parsing macros\materializations\view\view.sql
2019-02-08 09:50:38,928 (MainThread): Parsing macros\operations\catalog\get_catalog.sql
2019-02-08 09:50:38,928 (MainThread): Parsing macros\operations\relations\get_relations.sql
2019-02-08 09:50:38,929 (MainThread): Parsing macros\relations\postgres_relations.sql
2019-02-08 09:50:38,931 (MainThread): Parsing macros\relations\redshift_relations.sql
2019-02-08 09:50:38,932 (MainThread): Parsing macros\schema_tests\accepted_values.sql
2019-02-08 09:50:38,934 (MainThread): Parsing macros\schema_tests\not_null.sql
2019-02-08 09:50:38,935 (MainThread): Parsing macros\schema_tests\relationships.sql
2019-02-08 09:50:38,936 (MainThread): Parsing macros\schema_tests\unique.sql
2019-02-08 09:50:38,943 (MainThread): Parsing macros\core.sql
2019-02-08 09:50:38,946 (MainThread): Parsing macros\adapters\bigquery.sql
2019-02-08 09:50:38,950 (MainThread): Parsing macros\adapters\common.sql
2019-02-08 09:50:38,957 (MainThread): Parsing macros\adapters\redshift.sql
2019-02-08 09:50:38,963 (MainThread): Parsing macros\adapters\snowflake.sql
2019-02-08 09:50:38,964 (MainThread): Parsing macros\catalog\postgres_catalog.sql
2019-02-08 09:50:38,966 (MainThread): Parsing macros\catalog\redshift_catalog.sql
2019-02-08 09:50:38,971 (MainThread): Parsing macros\catalog\snowflake_catalog.sql
2019-02-08 09:50:38,973 (MainThread): Parsing macros\etc\bigquery.sql
2019-02-08 09:50:38,974 (MainThread): Parsing macros\etc\datetime.sql
2019-02-08 09:50:38,980 (MainThread): Parsing macros\etc\get_custom_schema.sql
2019-02-08 09:50:38,982 (MainThread): Parsing macros\materializations\helpers.sql
2019-02-08 09:50:38,987 (MainThread): Parsing macros\materializations\archive\archive.sql
2019-02-08 09:50:39,006 (MainThread): Parsing macros\materializations\common\merge.sql
2019-02-08 09:50:39,013 (MainThread): Parsing macros\materializations\incremental\bigquery_incremental.sql
2019-02-08 09:50:39,019 (MainThread): Parsing macros\materializations\incremental\incremental.sql
2019-02-08 09:50:39,027 (MainThread): Parsing macros\materializations\seed\bigquery.sql
2019-02-08 09:50:39,029 (MainThread): Parsing macros\materializations\seed\seed.sql
2019-02-08 09:50:39,041 (MainThread): Parsing macros\materializations\table\bigquery_table.sql
2019-02-08 09:50:39,049 (MainThread): Parsing macros\materializations\table\snowflake_table.sql
2019-02-08 09:50:39,057 (MainThread): Parsing macros\materializations\table\table.sql
2019-02-08 09:50:39,065 (MainThread): Parsing macros\materializations\view\bq_snowflake_view.sql
2019-02-08 09:50:39,072 (MainThread): Parsing macros\materializations\view\view.sql
2019-02-08 09:50:39,078 (MainThread): Parsing macros\operations\catalog\get_catalog.sql
2019-02-08 09:50:39,079 (MainThread): Parsing macros\operations\relations\get_relations.sql
2019-02-08 09:50:39,081 (MainThread): Parsing macros\relations\postgres_relations.sql
2019-02-08 09:50:39,082 (MainThread): Parsing macros\relations\redshift_relations.sql
2019-02-08 09:50:39,083 (MainThread): Parsing macros\schema_tests\accepted_values.sql
2019-02-08 09:50:39,084 (MainThread): Parsing macros\schema_tests\not_null.sql
2019-02-08 09:50:39,085 (MainThread): Parsing macros\schema_tests\relationships.sql
2019-02-08 09:50:39,087 (MainThread): Parsing macros\schema_tests\unique.sql
2019-02-08 09:50:39,088 (MainThread): Parsing model.dbt_learn.property_type_by_zipcode
2019-02-08 09:50:39,126 (MainThread): Found 1 models, 0 tests, 0 archives, 0 analyses, 94 macros, 2 operations, 0 seed files
2019-02-08 09:50:39,128 (MainThread): 
2019-02-08 09:50:39,128 (MainThread): Acquiring new redshift connection "master".
2019-02-08 09:50:39,128 (MainThread): Opening a new connection (0 currently allocated)
2019-02-08 09:50:39,129 (MainThread): Connecting to Redshift using 'database' credentials
2019-02-08 09:50:39,844 (MainThread): Using redshift connection "master".
2019-02-08 09:50:39,845 (MainThread): On master: select tablename as name, schemaname as schema, 'table' as type from pg_tables
        where schemaname ilike 'dbt_alex'
        union all
        select viewname as name, schemaname as schema, 'view' as type from pg_views
        where schemaname ilike 'dbt_alex'
2019-02-08 09:50:39,903 (MainThread): SQL status: SELECT in 0.06 seconds
2019-02-08 09:50:39,940 (MainThread): Acquiring new redshift connection "get_relations_data".
2019-02-08 09:50:39,940 (MainThread): Opening a new connection (1 currently allocated)
2019-02-08 09:50:39,941 (MainThread): Connecting to Redshift using 'database' credentials
2019-02-08 09:50:40,072 (MainThread): Using redshift connection "get_relations_data".
2019-02-08 09:50:40,072 (MainThread): On get_relations_data: BEGIN
2019-02-08 09:50:40,111 (MainThread): SQL status: BEGIN in 0.04 seconds
2019-02-08 09:50:40,112 (MainThread): Using redshift connection "get_relations_data".
2019-02-08 09:50:40,112 (MainThread): On get_relations_data: -- 
    with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg_%'
    ),
    relationships as (
        select
            referenced_class.name as referenced_name,
            referenced_class.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced_class.kind as kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        join dependency on relation.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            referenced_class.kind in ('r', 'v') and
            (referenced_class.name != dependent_class.name or
             referenced_class.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2019-02-08 09:50:40,157 (MainThread): SQL status: SELECT in 0.04 seconds
2019-02-08 09:50:40,164 (MainThread): On get_relations_data: ROLLBACK
2019-02-08 09:50:40,183 (MainThread): Using redshift connection "master".
2019-02-08 09:50:40,183 (MainThread): On master: select distinct nspname from pg_namespace
2019-02-08 09:50:40,203 (MainThread): SQL status: SELECT in 0.02 seconds
2019-02-08 09:50:40,203 (MainThread): 09:50:40 | Concurrency: 4 threads (target='dev')
2019-02-08 09:50:40,203 (MainThread): 09:50:40 | 
2019-02-08 09:50:40,206 (Thread-1): 09:50:40 | 1 of 1 START view model dbt_alex.property_type_by_zipcode............ [RUN]
2019-02-08 09:50:40,207 (Thread-1): Compiling model.dbt_learn.property_type_by_zipcode
2019-02-08 09:50:40,211 (Thread-1): Writing injected SQL for node "model.dbt_learn.property_type_by_zipcode"
2019-02-08 09:50:40,248 (Thread-1): Acquiring new redshift connection "property_type_by_zipcode".
2019-02-08 09:50:40,249 (Thread-1): Re-using an available connection from the pool.
2019-02-08 09:50:40,249 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:50:40,249 (Thread-1): On property_type_by_zipcode: BEGIN
2019-02-08 09:50:40,286 (Thread-1): SQL status: BEGIN in 0.04 seconds
2019-02-08 09:50:40,287 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:50:40,287 (Thread-1): On property_type_by_zipcode: drop view if exists "dbt_alex"."property_type_by_zipcode__dbt_tmp" cascade
2019-02-08 09:50:40,304 (Thread-1): SQL status: DROP VIEW in 0.02 seconds
2019-02-08 09:50:40,304 (Thread-1): On property_type_by_zipcode: COMMIT
2019-02-08 09:50:40,304 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:50:40,304 (Thread-1): On property_type_by_zipcode: COMMIT
2019-02-08 09:50:40,397 (Thread-1): SQL status: COMMIT in 0.09 seconds
2019-02-08 09:50:40,397 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:50:40,397 (Thread-1): On property_type_by_zipcode: BEGIN
2019-02-08 09:50:40,416 (Thread-1): SQL status: BEGIN in 0.02 seconds
2019-02-08 09:50:40,416 (Thread-1): On property_type_by_zipcode: COMMIT
2019-02-08 09:50:40,416 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:50:40,416 (Thread-1): On property_type_by_zipcode: COMMIT
2019-02-08 09:50:40,435 (Thread-1): SQL status: COMMIT in 0.02 seconds
2019-02-08 09:50:40,435 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:50:40,435 (Thread-1): On property_type_by_zipcode: BEGIN
2019-02-08 09:50:40,452 (Thread-1): SQL status: BEGIN in 0.02 seconds
2019-02-08 09:50:40,452 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:50:40,453 (Thread-1): On property_type_by_zipcode: drop view if exists "dbt_alex"."property_type_by_zipcode__dbt_backup" cascade
2019-02-08 09:50:40,472 (Thread-1): SQL status: DROP VIEW in 0.02 seconds
2019-02-08 09:50:40,472 (Thread-1): On property_type_by_zipcode: COMMIT
2019-02-08 09:50:40,472 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:50:40,472 (Thread-1): On property_type_by_zipcode: COMMIT
2019-02-08 09:50:40,656 (Thread-1): SQL status: COMMIT in 0.18 seconds
2019-02-08 09:50:40,656 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:50:40,656 (Thread-1): On property_type_by_zipcode: BEGIN
2019-02-08 09:50:40,671 (Thread-1): SQL status: BEGIN in 0.01 seconds
2019-02-08 09:50:40,685 (Thread-1): Writing runtime SQL for node "model.dbt_learn.property_type_by_zipcode"
2019-02-08 09:50:40,687 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:50:40,687 (Thread-1): On property_type_by_zipcode: 

  create view "dbt_alex"."property_type_by_zipcode__dbt_tmp" as (
    select
    zipcode
    
    -- writing sql using jinja instead of repeating the case when clause
    sum(case when lower(property_type) = 'house' then 1 else 0 end) as house_count,
    -- writing sql using jinja instead of repeating the case when clause
    sum(case when lower(property_type) = 'apartment' then 1 else 0 end) as apartment_count,
    -- writing sql using jinja instead of repeating the case when clause
    sum(case when lower(property_type) = 'townhouse' then 1 else 0 end) as townhouse_count

from source_data.listings

group by 1

limit 100
  ) ;
2019-02-08 09:50:40,705 (Thread-1): Postgres error: syntax error at or near "("
LINE 8:     sum(case when lower(property_type) = 'house' then 1 else...
               ^

2019-02-08 09:50:40,706 (Thread-1): On property_type_by_zipcode: ROLLBACK
2019-02-08 09:50:40,723 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bdc83253-31a5-481e-aa1d-ba7435741b51', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000010DA5D820F0>]}
2019-02-08 09:50:40,807 (Thread-1): 09:50:40 | 1 of 1 ERROR creating view model dbt_alex.property_type_by_zipcode... [ERROR in 0.52s]
2019-02-08 09:50:40,910 (MainThread): 09:50:40 | 
2019-02-08 09:50:40,911 (MainThread): 09:50:40 | Finished running 1 view models in 1.78s.
2019-02-08 09:50:40,912 (MainThread): Connection 'master' was left open.
2019-02-08 09:50:40,916 (MainThread): 
2019-02-08 09:50:40,917 (MainThread): Completed with 1 errors:
2019-02-08 09:50:40,918 (MainThread): 
2019-02-08 09:50:40,919 (MainThread): Database Error in model property_type_by_zipcode (models\property_type_by_zipcode.sql)
2019-02-08 09:50:40,920 (MainThread):   syntax error at or near "("
2019-02-08 09:50:40,922 (MainThread):   LINE 8:     sum(case when lower(property_type) = 'house' then 1 else...
2019-02-08 09:50:40,923 (MainThread):                  ^
2019-02-08 09:50:40,924 (MainThread):   compiled SQL at target\compiled\dbt_learn\property_type_by_zipcode.sql
2019-02-08 09:50:40,925 (MainThread): 
Done. PASS=0 ERROR=1 SKIP=0 TOTAL=1
2019-02-08 09:50:40,926 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000010DA5C07AC8>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000010DA23BB588>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000010DA5D77320>]}
2019-02-08 09:50:41,011 (MainThread): Flushing usage events
2019-02-08 09:52:09,595 (MainThread): Tracking: tracking
2019-02-08 09:52:09,596 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026981EC0B00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026981EC0CF8>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026981EC0860>]}
2019-02-08 09:52:09,772 (MainThread): Loading dependency project from c:\users\yichy\appdata\local\programs\python\python36\lib\site-packages\dbt\include
2019-02-08 09:52:09,779 (MainThread): Loading dependency project from C:\users\yichy\dbt-learn\dbt_modules
2019-02-08 09:52:09,785 (MainThread): Parsing macros\core.sql
2019-02-08 09:52:09,792 (MainThread): Parsing macros\adapters\bigquery.sql
2019-02-08 09:52:09,797 (MainThread): Parsing macros\adapters\common.sql
2019-02-08 09:52:09,809 (MainThread): Parsing macros\adapters\redshift.sql
2019-02-08 09:52:09,817 (MainThread): Parsing macros\adapters\snowflake.sql
2019-02-08 09:52:09,819 (MainThread): Parsing macros\catalog\postgres_catalog.sql
2019-02-08 09:52:09,821 (MainThread): Parsing macros\catalog\redshift_catalog.sql
2019-02-08 09:52:09,829 (MainThread): Parsing macros\catalog\snowflake_catalog.sql
2019-02-08 09:52:09,832 (MainThread): Parsing macros\etc\bigquery.sql
2019-02-08 09:52:09,833 (MainThread): Parsing macros\etc\datetime.sql
2019-02-08 09:52:09,842 (MainThread): Parsing macros\etc\get_custom_schema.sql
2019-02-08 09:52:09,845 (MainThread): Parsing macros\materializations\helpers.sql
2019-02-08 09:52:09,853 (MainThread): Parsing macros\materializations\archive\archive.sql
2019-02-08 09:52:09,879 (MainThread): Parsing macros\materializations\common\merge.sql
2019-02-08 09:52:09,884 (MainThread): Parsing macros\materializations\incremental\bigquery_incremental.sql
2019-02-08 09:52:09,890 (MainThread): Parsing macros\materializations\incremental\incremental.sql
2019-02-08 09:52:09,901 (MainThread): Parsing macros\materializations\seed\bigquery.sql
2019-02-08 09:52:09,904 (MainThread): Parsing macros\materializations\seed\seed.sql
2019-02-08 09:52:09,920 (MainThread): Parsing macros\materializations\table\bigquery_table.sql
2019-02-08 09:52:09,930 (MainThread): Parsing macros\materializations\table\snowflake_table.sql
2019-02-08 09:52:09,939 (MainThread): Parsing macros\materializations\table\table.sql
2019-02-08 09:52:09,949 (MainThread): Parsing macros\materializations\view\bq_snowflake_view.sql
2019-02-08 09:52:09,959 (MainThread): Parsing macros\materializations\view\view.sql
2019-02-08 09:52:09,967 (MainThread): Parsing macros\operations\catalog\get_catalog.sql
2019-02-08 09:52:09,968 (MainThread): Parsing macros\operations\relations\get_relations.sql
2019-02-08 09:52:09,969 (MainThread): Parsing macros\relations\postgres_relations.sql
2019-02-08 09:52:09,971 (MainThread): Parsing macros\relations\redshift_relations.sql
2019-02-08 09:52:09,972 (MainThread): Parsing macros\schema_tests\accepted_values.sql
2019-02-08 09:52:09,975 (MainThread): Parsing macros\schema_tests\not_null.sql
2019-02-08 09:52:09,976 (MainThread): Parsing macros\schema_tests\relationships.sql
2019-02-08 09:52:09,978 (MainThread): Parsing macros\schema_tests\unique.sql
2019-02-08 09:52:09,986 (MainThread): Parsing macros\core.sql
2019-02-08 09:52:09,990 (MainThread): Parsing macros\adapters\bigquery.sql
2019-02-08 09:52:09,994 (MainThread): Parsing macros\adapters\common.sql
2019-02-08 09:52:10,003 (MainThread): Parsing macros\adapters\redshift.sql
2019-02-08 09:52:10,008 (MainThread): Parsing macros\adapters\snowflake.sql
2019-02-08 09:52:10,010 (MainThread): Parsing macros\catalog\postgres_catalog.sql
2019-02-08 09:52:10,011 (MainThread): Parsing macros\catalog\redshift_catalog.sql
2019-02-08 09:52:10,017 (MainThread): Parsing macros\catalog\snowflake_catalog.sql
2019-02-08 09:52:10,019 (MainThread): Parsing macros\etc\bigquery.sql
2019-02-08 09:52:10,020 (MainThread): Parsing macros\etc\datetime.sql
2019-02-08 09:52:10,028 (MainThread): Parsing macros\etc\get_custom_schema.sql
2019-02-08 09:52:10,031 (MainThread): Parsing macros\materializations\helpers.sql
2019-02-08 09:52:10,036 (MainThread): Parsing macros\materializations\archive\archive.sql
2019-02-08 09:52:10,058 (MainThread): Parsing macros\materializations\common\merge.sql
2019-02-08 09:52:10,062 (MainThread): Parsing macros\materializations\incremental\bigquery_incremental.sql
2019-02-08 09:52:10,068 (MainThread): Parsing macros\materializations\incremental\incremental.sql
2019-02-08 09:52:10,078 (MainThread): Parsing macros\materializations\seed\bigquery.sql
2019-02-08 09:52:10,080 (MainThread): Parsing macros\materializations\seed\seed.sql
2019-02-08 09:52:10,094 (MainThread): Parsing macros\materializations\table\bigquery_table.sql
2019-02-08 09:52:10,104 (MainThread): Parsing macros\materializations\table\snowflake_table.sql
2019-02-08 09:52:10,113 (MainThread): Parsing macros\materializations\table\table.sql
2019-02-08 09:52:10,122 (MainThread): Parsing macros\materializations\view\bq_snowflake_view.sql
2019-02-08 09:52:10,130 (MainThread): Parsing macros\materializations\view\view.sql
2019-02-08 09:52:10,138 (MainThread): Parsing macros\operations\catalog\get_catalog.sql
2019-02-08 09:52:10,139 (MainThread): Parsing macros\operations\relations\get_relations.sql
2019-02-08 09:52:10,140 (MainThread): Parsing macros\relations\postgres_relations.sql
2019-02-08 09:52:10,142 (MainThread): Parsing macros\relations\redshift_relations.sql
2019-02-08 09:52:10,143 (MainThread): Parsing macros\schema_tests\accepted_values.sql
2019-02-08 09:52:10,145 (MainThread): Parsing macros\schema_tests\not_null.sql
2019-02-08 09:52:10,147 (MainThread): Parsing macros\schema_tests\relationships.sql
2019-02-08 09:52:10,148 (MainThread): Parsing macros\schema_tests\unique.sql
2019-02-08 09:52:10,150 (MainThread): Parsing model.dbt_learn.property_type_by_zipcode
2019-02-08 09:52:10,192 (MainThread): Found 1 models, 0 tests, 0 archives, 0 analyses, 94 macros, 2 operations, 0 seed files
2019-02-08 09:52:10,194 (MainThread): 
2019-02-08 09:52:10,194 (MainThread): Acquiring new redshift connection "master".
2019-02-08 09:52:10,194 (MainThread): Opening a new connection (0 currently allocated)
2019-02-08 09:52:10,195 (MainThread): Connecting to Redshift using 'database' credentials
2019-02-08 09:52:10,913 (MainThread): Using redshift connection "master".
2019-02-08 09:52:10,913 (MainThread): On master: select tablename as name, schemaname as schema, 'table' as type from pg_tables
        where schemaname ilike 'dbt_alex'
        union all
        select viewname as name, schemaname as schema, 'view' as type from pg_views
        where schemaname ilike 'dbt_alex'
2019-02-08 09:52:10,966 (MainThread): SQL status: SELECT in 0.05 seconds
2019-02-08 09:52:11,005 (MainThread): Acquiring new redshift connection "get_relations_data".
2019-02-08 09:52:11,005 (MainThread): Opening a new connection (1 currently allocated)
2019-02-08 09:52:11,006 (MainThread): Connecting to Redshift using 'database' credentials
2019-02-08 09:52:11,147 (MainThread): Using redshift connection "get_relations_data".
2019-02-08 09:52:11,147 (MainThread): On get_relations_data: BEGIN
2019-02-08 09:52:11,183 (MainThread): SQL status: BEGIN in 0.04 seconds
2019-02-08 09:52:11,183 (MainThread): Using redshift connection "get_relations_data".
2019-02-08 09:52:11,184 (MainThread): On get_relations_data: -- 
    with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg_%'
    ),
    relationships as (
        select
            referenced_class.name as referenced_name,
            referenced_class.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced_class.kind as kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        join dependency on relation.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            referenced_class.kind in ('r', 'v') and
            (referenced_class.name != dependent_class.name or
             referenced_class.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2019-02-08 09:52:11,232 (MainThread): SQL status: SELECT in 0.05 seconds
2019-02-08 09:52:11,241 (MainThread): On get_relations_data: ROLLBACK
2019-02-08 09:52:11,262 (MainThread): Using redshift connection "master".
2019-02-08 09:52:11,262 (MainThread): On master: select distinct nspname from pg_namespace
2019-02-08 09:52:11,279 (MainThread): SQL status: SELECT in 0.02 seconds
2019-02-08 09:52:11,280 (MainThread): 09:52:11 | Concurrency: 4 threads (target='dev')
2019-02-08 09:52:11,280 (MainThread): 09:52:11 | 
2019-02-08 09:52:11,284 (Thread-1): 09:52:11 | 1 of 1 START view model dbt_alex.property_type_by_zipcode............ [RUN]
2019-02-08 09:52:11,286 (Thread-1): Compiling model.dbt_learn.property_type_by_zipcode
2019-02-08 09:52:11,297 (Thread-1): Writing injected SQL for node "model.dbt_learn.property_type_by_zipcode"
2019-02-08 09:52:11,342 (Thread-1): Acquiring new redshift connection "property_type_by_zipcode".
2019-02-08 09:52:11,342 (Thread-1): Re-using an available connection from the pool.
2019-02-08 09:52:11,343 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:52:11,343 (Thread-1): On property_type_by_zipcode: BEGIN
2019-02-08 09:52:11,382 (Thread-1): SQL status: BEGIN in 0.04 seconds
2019-02-08 09:52:11,383 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:52:11,383 (Thread-1): On property_type_by_zipcode: drop view if exists "dbt_alex"."property_type_by_zipcode__dbt_tmp" cascade
2019-02-08 09:52:11,404 (Thread-1): SQL status: DROP VIEW in 0.02 seconds
2019-02-08 09:52:11,404 (Thread-1): On property_type_by_zipcode: COMMIT
2019-02-08 09:52:11,404 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:52:11,404 (Thread-1): On property_type_by_zipcode: COMMIT
2019-02-08 09:52:11,631 (Thread-1): SQL status: COMMIT in 0.23 seconds
2019-02-08 09:52:11,632 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:52:11,632 (Thread-1): On property_type_by_zipcode: BEGIN
2019-02-08 09:52:11,651 (Thread-1): SQL status: BEGIN in 0.02 seconds
2019-02-08 09:52:11,652 (Thread-1): On property_type_by_zipcode: COMMIT
2019-02-08 09:52:11,652 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:52:11,652 (Thread-1): On property_type_by_zipcode: COMMIT
2019-02-08 09:52:11,672 (Thread-1): SQL status: COMMIT in 0.02 seconds
2019-02-08 09:52:11,673 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:52:11,673 (Thread-1): On property_type_by_zipcode: BEGIN
2019-02-08 09:52:11,691 (Thread-1): SQL status: BEGIN in 0.02 seconds
2019-02-08 09:52:11,692 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:52:11,692 (Thread-1): On property_type_by_zipcode: drop view if exists "dbt_alex"."property_type_by_zipcode__dbt_backup" cascade
2019-02-08 09:52:11,714 (Thread-1): SQL status: DROP VIEW in 0.02 seconds
2019-02-08 09:52:11,715 (Thread-1): On property_type_by_zipcode: COMMIT
2019-02-08 09:52:11,715 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:52:11,715 (Thread-1): On property_type_by_zipcode: COMMIT
2019-02-08 09:52:11,863 (Thread-1): SQL status: COMMIT in 0.15 seconds
2019-02-08 09:52:11,863 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:52:11,863 (Thread-1): On property_type_by_zipcode: BEGIN
2019-02-08 09:52:11,883 (Thread-1): SQL status: BEGIN in 0.02 seconds
2019-02-08 09:52:11,898 (Thread-1): Writing runtime SQL for node "model.dbt_learn.property_type_by_zipcode"
2019-02-08 09:52:11,900 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:52:11,901 (Thread-1): On property_type_by_zipcode: 

  create view "dbt_alex"."property_type_by_zipcode__dbt_tmp" as (
    select
    zipcode
    
    -- writing sql using jinja instead of repeating the case when clause
    sum(case when lower(property_type) = 'house' then 1 else 0 end) as house_count,
    -- writing sql using jinja instead of repeating the case when clause
    sum(case when lower(property_type) = 'apartment' then 1 else 0 end) as apartment_count,
    -- writing sql using jinja instead of repeating the case when clause
    sum(case when lower(property_type) = 'townhouse' then 1 else 0 end) as townhouse_count

from source_data.listings

group by 1

limit 100
  ) ;
2019-02-08 09:52:11,921 (Thread-1): Postgres error: syntax error at or near "("
LINE 8:     sum(case when lower(property_type) = 'house' then 1 else...
               ^

2019-02-08 09:52:11,921 (Thread-1): On property_type_by_zipcode: ROLLBACK
2019-02-08 09:52:11,944 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2a42a460-ce04-4cd9-9df0-f8ebe8783faa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026981F07630>]}
2019-02-08 09:52:12,044 (Thread-1): 09:52:12 | 1 of 1 ERROR creating view model dbt_alex.property_type_by_zipcode... [ERROR in 0.66s]
2019-02-08 09:52:12,095 (MainThread): 09:52:12 | 
2019-02-08 09:52:12,095 (MainThread): 09:52:12 | Finished running 1 view models in 1.90s.
2019-02-08 09:52:12,096 (MainThread): Connection 'master' was left open.
2019-02-08 09:52:12,101 (MainThread): 
2019-02-08 09:52:12,102 (MainThread): Completed with 1 errors:
2019-02-08 09:52:12,103 (MainThread): 
2019-02-08 09:52:12,103 (MainThread): Database Error in model property_type_by_zipcode (models\property_type_by_zipcode.sql)
2019-02-08 09:52:12,104 (MainThread):   syntax error at or near "("
2019-02-08 09:52:12,105 (MainThread):   LINE 8:     sum(case when lower(property_type) = 'house' then 1 else...
2019-02-08 09:52:12,106 (MainThread):                  ^
2019-02-08 09:52:12,107 (MainThread):   compiled SQL at target\compiled\dbt_learn\property_type_by_zipcode.sql
2019-02-08 09:52:12,108 (MainThread): 
Done. PASS=0 ERROR=1 SKIP=0 TOTAL=1
2019-02-08 09:52:12,109 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026981DA4390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000269802A4518>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026981EC0F98>]}
2019-02-08 09:52:12,200 (MainThread): Flushing usage events
2019-02-08 09:52:29,305 (MainThread): Tracking: tracking
2019-02-08 09:52:29,305 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000210D3C60B38>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000210D3C60D30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000210D3C60898>]}
2019-02-08 09:52:29,414 (MainThread): Loading dependency project from c:\users\yichy\appdata\local\programs\python\python36\lib\site-packages\dbt\include
2019-02-08 09:52:29,421 (MainThread): Loading dependency project from C:\users\yichy\dbt-learn\dbt_modules
2019-02-08 09:52:29,427 (MainThread): Parsing macros\core.sql
2019-02-08 09:52:29,433 (MainThread): Parsing macros\adapters\bigquery.sql
2019-02-08 09:52:29,438 (MainThread): Parsing macros\adapters\common.sql
2019-02-08 09:52:29,450 (MainThread): Parsing macros\adapters\redshift.sql
2019-02-08 09:52:29,458 (MainThread): Parsing macros\adapters\snowflake.sql
2019-02-08 09:52:29,460 (MainThread): Parsing macros\catalog\postgres_catalog.sql
2019-02-08 09:52:29,462 (MainThread): Parsing macros\catalog\redshift_catalog.sql
2019-02-08 09:52:29,470 (MainThread): Parsing macros\catalog\snowflake_catalog.sql
2019-02-08 09:52:29,472 (MainThread): Parsing macros\etc\bigquery.sql
2019-02-08 09:52:29,473 (MainThread): Parsing macros\etc\datetime.sql
2019-02-08 09:52:29,480 (MainThread): Parsing macros\etc\get_custom_schema.sql
2019-02-08 09:52:29,483 (MainThread): Parsing macros\materializations\helpers.sql
2019-02-08 09:52:29,491 (MainThread): Parsing macros\materializations\archive\archive.sql
2019-02-08 09:52:29,516 (MainThread): Parsing macros\materializations\common\merge.sql
2019-02-08 09:52:29,521 (MainThread): Parsing macros\materializations\incremental\bigquery_incremental.sql
2019-02-08 09:52:29,527 (MainThread): Parsing macros\materializations\incremental\incremental.sql
2019-02-08 09:52:29,537 (MainThread): Parsing macros\materializations\seed\bigquery.sql
2019-02-08 09:52:29,540 (MainThread): Parsing macros\materializations\seed\seed.sql
2019-02-08 09:52:29,556 (MainThread): Parsing macros\materializations\table\bigquery_table.sql
2019-02-08 09:52:29,565 (MainThread): Parsing macros\materializations\table\snowflake_table.sql
2019-02-08 09:52:29,574 (MainThread): Parsing macros\materializations\table\table.sql
2019-02-08 09:52:29,583 (MainThread): Parsing macros\materializations\view\bq_snowflake_view.sql
2019-02-08 09:52:29,593 (MainThread): Parsing macros\materializations\view\view.sql
2019-02-08 09:52:29,601 (MainThread): Parsing macros\operations\catalog\get_catalog.sql
2019-02-08 09:52:29,602 (MainThread): Parsing macros\operations\relations\get_relations.sql
2019-02-08 09:52:29,603 (MainThread): Parsing macros\relations\postgres_relations.sql
2019-02-08 09:52:29,605 (MainThread): Parsing macros\relations\redshift_relations.sql
2019-02-08 09:52:29,607 (MainThread): Parsing macros\schema_tests\accepted_values.sql
2019-02-08 09:52:29,609 (MainThread): Parsing macros\schema_tests\not_null.sql
2019-02-08 09:52:29,610 (MainThread): Parsing macros\schema_tests\relationships.sql
2019-02-08 09:52:29,612 (MainThread): Parsing macros\schema_tests\unique.sql
2019-02-08 09:52:29,618 (MainThread): Parsing macros\core.sql
2019-02-08 09:52:29,622 (MainThread): Parsing macros\adapters\bigquery.sql
2019-02-08 09:52:29,626 (MainThread): Parsing macros\adapters\common.sql
2019-02-08 09:52:29,635 (MainThread): Parsing macros\adapters\redshift.sql
2019-02-08 09:52:29,642 (MainThread): Parsing macros\adapters\snowflake.sql
2019-02-08 09:52:29,644 (MainThread): Parsing macros\catalog\postgres_catalog.sql
2019-02-08 09:52:29,646 (MainThread): Parsing macros\catalog\redshift_catalog.sql
2019-02-08 09:52:29,651 (MainThread): Parsing macros\catalog\snowflake_catalog.sql
2019-02-08 09:52:29,652 (MainThread): Parsing macros\etc\bigquery.sql
2019-02-08 09:52:29,653 (MainThread): Parsing macros\etc\datetime.sql
2019-02-08 09:52:29,661 (MainThread): Parsing macros\etc\get_custom_schema.sql
2019-02-08 09:52:29,664 (MainThread): Parsing macros\materializations\helpers.sql
2019-02-08 09:52:29,669 (MainThread): Parsing macros\materializations\archive\archive.sql
2019-02-08 09:52:29,690 (MainThread): Parsing macros\materializations\common\merge.sql
2019-02-08 09:52:29,694 (MainThread): Parsing macros\materializations\incremental\bigquery_incremental.sql
2019-02-08 09:52:29,700 (MainThread): Parsing macros\materializations\incremental\incremental.sql
2019-02-08 09:52:29,710 (MainThread): Parsing macros\materializations\seed\bigquery.sql
2019-02-08 09:52:29,712 (MainThread): Parsing macros\materializations\seed\seed.sql
2019-02-08 09:52:29,725 (MainThread): Parsing macros\materializations\table\bigquery_table.sql
2019-02-08 09:52:29,734 (MainThread): Parsing macros\materializations\table\snowflake_table.sql
2019-02-08 09:52:29,743 (MainThread): Parsing macros\materializations\table\table.sql
2019-02-08 09:52:29,752 (MainThread): Parsing macros\materializations\view\bq_snowflake_view.sql
2019-02-08 09:52:29,760 (MainThread): Parsing macros\materializations\view\view.sql
2019-02-08 09:52:29,767 (MainThread): Parsing macros\operations\catalog\get_catalog.sql
2019-02-08 09:52:29,768 (MainThread): Parsing macros\operations\relations\get_relations.sql
2019-02-08 09:52:29,770 (MainThread): Parsing macros\relations\postgres_relations.sql
2019-02-08 09:52:29,772 (MainThread): Parsing macros\relations\redshift_relations.sql
2019-02-08 09:52:29,772 (MainThread): Parsing macros\schema_tests\accepted_values.sql
2019-02-08 09:52:29,774 (MainThread): Parsing macros\schema_tests\not_null.sql
2019-02-08 09:52:29,775 (MainThread): Parsing macros\schema_tests\relationships.sql
2019-02-08 09:52:29,777 (MainThread): Parsing macros\schema_tests\unique.sql
2019-02-08 09:52:29,779 (MainThread): Parsing model.dbt_learn.property_type_by_zipcode
2019-02-08 09:52:29,822 (MainThread): Found 1 models, 0 tests, 0 archives, 0 analyses, 94 macros, 2 operations, 0 seed files
2019-02-08 09:52:29,824 (MainThread): 
2019-02-08 09:52:29,824 (MainThread): Acquiring new redshift connection "master".
2019-02-08 09:52:29,824 (MainThread): Opening a new connection (0 currently allocated)
2019-02-08 09:52:29,825 (MainThread): Connecting to Redshift using 'database' credentials
2019-02-08 09:52:30,528 (MainThread): Using redshift connection "master".
2019-02-08 09:52:30,528 (MainThread): On master: select tablename as name, schemaname as schema, 'table' as type from pg_tables
        where schemaname ilike 'dbt_alex'
        union all
        select viewname as name, schemaname as schema, 'view' as type from pg_views
        where schemaname ilike 'dbt_alex'
2019-02-08 09:52:30,574 (MainThread): SQL status: SELECT in 0.05 seconds
2019-02-08 09:52:30,612 (MainThread): Acquiring new redshift connection "get_relations_data".
2019-02-08 09:52:30,612 (MainThread): Opening a new connection (1 currently allocated)
2019-02-08 09:52:30,613 (MainThread): Connecting to Redshift using 'database' credentials
2019-02-08 09:52:30,748 (MainThread): Using redshift connection "get_relations_data".
2019-02-08 09:52:30,748 (MainThread): On get_relations_data: BEGIN
2019-02-08 09:52:30,783 (MainThread): SQL status: BEGIN in 0.03 seconds
2019-02-08 09:52:30,784 (MainThread): Using redshift connection "get_relations_data".
2019-02-08 09:52:30,784 (MainThread): On get_relations_data: -- 
    with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg_%'
    ),
    relationships as (
        select
            referenced_class.name as referenced_name,
            referenced_class.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced_class.kind as kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        join dependency on relation.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            referenced_class.kind in ('r', 'v') and
            (referenced_class.name != dependent_class.name or
             referenced_class.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2019-02-08 09:52:30,833 (MainThread): SQL status: SELECT in 0.05 seconds
2019-02-08 09:52:30,841 (MainThread): On get_relations_data: ROLLBACK
2019-02-08 09:52:30,859 (MainThread): Using redshift connection "master".
2019-02-08 09:52:30,859 (MainThread): On master: select distinct nspname from pg_namespace
2019-02-08 09:52:30,879 (MainThread): SQL status: SELECT in 0.02 seconds
2019-02-08 09:52:30,879 (MainThread): 09:52:30 | Concurrency: 4 threads (target='dev')
2019-02-08 09:52:30,880 (MainThread): 09:52:30 | 
2019-02-08 09:52:30,883 (Thread-1): 09:52:30 | 1 of 1 START view model dbt_alex.property_type_by_zipcode............ [RUN]
2019-02-08 09:52:30,884 (Thread-1): Compiling model.dbt_learn.property_type_by_zipcode
2019-02-08 09:52:30,890 (Thread-1): Writing injected SQL for node "model.dbt_learn.property_type_by_zipcode"
2019-02-08 09:52:30,923 (Thread-1): Acquiring new redshift connection "property_type_by_zipcode".
2019-02-08 09:52:30,923 (Thread-1): Re-using an available connection from the pool.
2019-02-08 09:52:30,924 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:52:30,924 (Thread-1): On property_type_by_zipcode: BEGIN
2019-02-08 09:52:30,958 (Thread-1): SQL status: BEGIN in 0.03 seconds
2019-02-08 09:52:30,958 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:52:30,958 (Thread-1): On property_type_by_zipcode: drop view if exists "dbt_alex"."property_type_by_zipcode__dbt_tmp" cascade
2019-02-08 09:52:30,978 (Thread-1): SQL status: DROP VIEW in 0.02 seconds
2019-02-08 09:52:30,978 (Thread-1): On property_type_by_zipcode: COMMIT
2019-02-08 09:52:30,978 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:52:30,978 (Thread-1): On property_type_by_zipcode: COMMIT
2019-02-08 09:52:31,060 (Thread-1): SQL status: COMMIT in 0.08 seconds
2019-02-08 09:52:31,060 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:52:31,060 (Thread-1): On property_type_by_zipcode: BEGIN
2019-02-08 09:52:31,080 (Thread-1): SQL status: BEGIN in 0.02 seconds
2019-02-08 09:52:31,081 (Thread-1): On property_type_by_zipcode: COMMIT
2019-02-08 09:52:31,081 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:52:31,081 (Thread-1): On property_type_by_zipcode: COMMIT
2019-02-08 09:52:31,097 (Thread-1): SQL status: COMMIT in 0.02 seconds
2019-02-08 09:52:31,098 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:52:31,098 (Thread-1): On property_type_by_zipcode: BEGIN
2019-02-08 09:52:31,114 (Thread-1): SQL status: BEGIN in 0.01 seconds
2019-02-08 09:52:31,114 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:52:31,114 (Thread-1): On property_type_by_zipcode: drop view if exists "dbt_alex"."property_type_by_zipcode__dbt_backup" cascade
2019-02-08 09:52:31,134 (Thread-1): SQL status: DROP VIEW in 0.02 seconds
2019-02-08 09:52:31,134 (Thread-1): On property_type_by_zipcode: COMMIT
2019-02-08 09:52:31,134 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:52:31,134 (Thread-1): On property_type_by_zipcode: COMMIT
2019-02-08 09:52:31,209 (Thread-1): SQL status: COMMIT in 0.07 seconds
2019-02-08 09:52:31,209 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:52:31,209 (Thread-1): On property_type_by_zipcode: BEGIN
2019-02-08 09:52:31,226 (Thread-1): SQL status: BEGIN in 0.02 seconds
2019-02-08 09:52:31,241 (Thread-1): Writing runtime SQL for node "model.dbt_learn.property_type_by_zipcode"
2019-02-08 09:52:31,243 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:52:31,243 (Thread-1): On property_type_by_zipcode: 

  create view "dbt_alex"."property_type_by_zipcode__dbt_tmp" as (
    select
    zipcode,
    
    -- writing sql using jinja instead of repeating the case when clause
    sum(case when lower(property_type) = 'house' then 1 else 0 end) as house_count,
    -- writing sql using jinja instead of repeating the case when clause
    sum(case when lower(property_type) = 'apartment' then 1 else 0 end) as apartment_count,
    -- writing sql using jinja instead of repeating the case when clause
    sum(case when lower(property_type) = 'townhouse' then 1 else 0 end) as townhouse_count

from source_data.listings

group by 1

limit 100
  ) ;
2019-02-08 09:52:31,269 (Thread-1): SQL status: CREATE VIEW in 0.02 seconds
2019-02-08 09:52:31,271 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:52:31,271 (Thread-1): On property_type_by_zipcode: alter table "dbt_alex"."property_type_by_zipcode__dbt_tmp" rename to "property_type_by_zipcode"
2019-02-08 09:52:31,288 (Thread-1): SQL status: ALTER TABLE in 0.02 seconds
2019-02-08 09:52:31,289 (Thread-1): On property_type_by_zipcode: COMMIT
2019-02-08 09:52:31,289 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:52:31,289 (Thread-1): On property_type_by_zipcode: COMMIT
2019-02-08 09:52:31,444 (Thread-1): SQL status: COMMIT in 0.16 seconds
2019-02-08 09:52:31,445 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:52:31,445 (Thread-1): On property_type_by_zipcode: BEGIN
2019-02-08 09:52:31,461 (Thread-1): SQL status: BEGIN in 0.02 seconds
2019-02-08 09:52:31,462 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:52:31,462 (Thread-1): On property_type_by_zipcode: drop view if exists "dbt_alex"."property_type_by_zipcode__dbt_backup" cascade
2019-02-08 09:52:31,480 (Thread-1): SQL status: DROP VIEW in 0.02 seconds
2019-02-08 09:52:31,480 (Thread-1): On property_type_by_zipcode: COMMIT
2019-02-08 09:52:31,480 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:52:31,480 (Thread-1): On property_type_by_zipcode: COMMIT
2019-02-08 09:52:31,563 (Thread-1): SQL status: COMMIT in 0.08 seconds
2019-02-08 09:52:31,563 (Thread-1): Using redshift connection "property_type_by_zipcode".
2019-02-08 09:52:31,563 (Thread-1): On property_type_by_zipcode: BEGIN
2019-02-08 09:52:31,580 (Thread-1): SQL status: BEGIN in 0.02 seconds
2019-02-08 09:52:31,582 (Thread-1): On property_type_by_zipcode: ROLLBACK
2019-02-08 09:52:31,599 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '994e7f76-d61c-461d-8a0d-159126d9bd0a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000210D3CA0390>]}
2019-02-08 09:52:31,686 (Thread-1): 09:52:31 | 1 of 1 OK created view model dbt_alex.property_type_by_zipcode....... [CREATE VIEW in 0.71s]
2019-02-08 09:52:31,689 (MainThread): 09:52:31 | 
2019-02-08 09:52:31,690 (MainThread): 09:52:31 | Finished running 1 view models in 1.87s.
2019-02-08 09:52:31,690 (MainThread): Connection 'master' was left open.
2019-02-08 09:52:31,696 (MainThread): 
2019-02-08 09:52:31,696 (MainThread): Completed successfully
2019-02-08 09:52:31,697 (MainThread): 
Done. PASS=1 ERROR=0 SKIP=0 TOTAL=1
2019-02-08 09:52:31,699 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000210D3CE47F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000210D13A93C8>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000210D3C1A1D0>]}
2019-02-08 09:52:31,794 (MainThread): Flushing usage events
2019-02-08 10:11:28,461 (MainThread): Tracking: tracking
2019-02-08 10:11:28,461 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014001812208>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014001812160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014001812400>]}
2019-02-08 10:11:28,671 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001407EE5B898>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001407CF85CF8>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001407EF86518>]}
2019-02-08 10:11:28,773 (MainThread): Flushing usage events
2019-02-08 10:21:28,749 (MainThread): Tracking: tracking
2019-02-08 10:21:28,749 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000298DE02FF98>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000298DE02FA90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000298DE02F0B8>]}
2019-02-08 10:21:29,258 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000298DE01A208>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000298DE01AB00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000298DE01AF28>]}
2019-02-08 10:21:29,343 (MainThread): Flushing usage events
2019-02-08 10:21:29,343 (MainThread): Encountered an error:
2019-02-08 10:21:29,344 (MainThread): 'dict' object has no attribute 'packages'
2019-02-08 10:21:29,346 (MainThread): Traceback (most recent call last):
  File "c:\users\yichy\appdata\local\programs\python\python36\lib\site-packages\dbt\main.py", line 76, in main
    results, succeeded = handle_and_check(args)
  File "c:\users\yichy\appdata\local\programs\python\python36\lib\site-packages\dbt\main.py", line 126, in handle_and_check
    task, res = run_from_args(parsed)
  File "c:\users\yichy\appdata\local\programs\python\python36\lib\site-packages\dbt\main.py", line 181, in run_from_args
    results = run_from_task(task, cfg, parsed)
  File "c:\users\yichy\appdata\local\programs\python\python36\lib\site-packages\dbt\main.py", line 189, in run_from_task
    result = task.run()
  File "c:\users\yichy\appdata\local\programs\python\python36\lib\site-packages\dbt\task\deps.py", line 440, in run
    sub_deps.incorporate_from_yaml(target_config.packages.packages)
AttributeError: 'dict' object has no attribute 'packages'

